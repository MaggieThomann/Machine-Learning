{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Assignment 3:  Decision Tree Implementation\n",
    "*Margaret Thomann - February 17, 2018 *\n",
    "\n",
    "In this assignment, I will construct a decision tree from the data provided about heart disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Reading the data and assigning counts to arrays and Data class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Data Class \n",
    "A Data class will be instantiated for each line of the data.  It will then be added to one of two arrays (explained later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3806,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class Data:\n",
    "    def __init__(self, class_value):\n",
    "        self.class_value = class_value\n",
    "        self.data_vars = OrderedDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Calculate Information Gain for Each Feature\n",
    "The below function can be used to determine the information gain for a given data and hypothesis (passed in as a string - x and y).  Information Gain can be represented as: Infgain(Y|X_K) = H(Y) - H(Y|X_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3807,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Data processed\n",
      "-----------------\n",
      "\t120 = # Of People with Heart Disease\n",
      "\t150 = # Of People without Heart Disease\n"
     ]
    }
   ],
   "source": [
    "# Arrays for the Data instances\n",
    "#     absence_heart_array  : contains all Data instantiations where heart disease is absent\n",
    "#     presence_heart_array : contains all Data instantiations where heart disease is absent\n",
    "absence_heart_array = []\n",
    "presence_heart_array = []\n",
    "total_data_array = []\n",
    "\n",
    "# Classify the features according to their type: nominal or continuous\n",
    "indices_for_nominal = [10, 1, 5, 8, 6, 2, 12]\n",
    "indices_for_continuous = [0,3,4,7,9,11]\n",
    "feature_names = [\"age\", \"sex\", \"chest_pain_type\", \"resting_blood_pressure\", \"serum_cholesterol\", \"fasting_blood_sugar\",\n",
    "                \"resting_electrocardiographic_results\", \"maximum_heart_rate_achieved\", \"exercise_induced_angina\",\n",
    "                \"oldpeak\", \"slope_peak_exercise\", \"number_of_major_vessels\", \"thal\", \"has_heart_disease\"]\n",
    "features_and_types = OrderedDict()\n",
    "for feature in feature_names:\n",
    "    if feature_names.index(feature) in indices_for_continuous:\n",
    "        features_and_types[feature] = \"continuous\"\n",
    "    else:\n",
    "        features_and_types[feature] = \"nominal\"\n",
    "\n",
    "# Process the data and store it in the arrays\n",
    "data = open('heart.data.txt')\n",
    "for line in data.readlines():\n",
    "    feature_value_list = line.split()\n",
    "    has_heart_disease = int(feature_value_list[-1])\n",
    "    data = Data(has_heart_disease)\n",
    "    counter = 0\n",
    "    feature_dict = OrderedDict()\n",
    "    for feature in feature_value_list:\n",
    "        data.data_vars[feature_names[counter]] = float(feature)\n",
    "        counter += 1\n",
    "    if has_heart_disease == 2:\n",
    "        presence_heart_array.append(data)\n",
    "    elif has_heart_disease == 1:\n",
    "        absence_heart_array.append(data)\n",
    "    total_data_array.append(data)\n",
    "\n",
    "presence_heart_array_num = len(presence_heart_array)\n",
    "absence_heart_array_num = len(absence_heart_array)\n",
    "print \"✔ Data processed\"\n",
    "print \"-----------------\"\n",
    "print \"\\t\" + str(presence_heart_array_num) + \" = # Of People with Heart Disease\"\n",
    "print \"\\t\" + str(absence_heart_array_num) + \" = # Of People without Heart Disease\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Printer Function to Print out Data\n",
    "The below function can be called with a Data object as its input to print out that data objects contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3808,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_data(data_obj):\n",
    "    print \"_____________\"\n",
    "    for feature in data_obj.data_vars.keys():\n",
    "        print data_obj.data_vars[feature], \": \", feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3809,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_info_gain(y, y_values, x, buckets, considered_data):\n",
    "        \n",
    "    '''\n",
    "    if x == \"sex\":\n",
    "        print \"sex vals!!!\"\n",
    "        for data_point in considered_data:\n",
    "            print data_point.data_vars[\"sex\"]\n",
    "        #print \"y_values:\"\n",
    "        #print y_values\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    if y == \"chest_pain_type\":\n",
    "        print \"y_values:\"\n",
    "        print y_values\n",
    "    '''\n",
    "    \n",
    "    #print \"Calling compute_info_gain with y = \"+y+\" and x = \"+x\n",
    "    \n",
    "        \n",
    "    # Define dicts for the counts\n",
    "    positive_y_counts = {}\n",
    "    positive_x_counts = {}\n",
    "    negative_y_counts = {}\n",
    "    negative_x_counts = {}\n",
    "    \n",
    "    # Get the bucket values\n",
    "    for bucket in buckets:\n",
    "        # Convert to string\n",
    "        s = \"\"\n",
    "        for num in list(set(bucket)):\n",
    "            s += (str(num)+ \" \")\n",
    "        positive_x_counts[s] = 0\n",
    "        negative_x_counts[s] = 0\n",
    "    \n",
    "    \n",
    "    for val in y_values:\n",
    "        positive_y_counts[str(float(val))] = 0\n",
    "        negative_y_counts[str(float(val))] = 0\n",
    "    \n",
    "    y_denom = 0\n",
    "    for data in considered_data:\n",
    "        y_denom += 1\n",
    "        has_heart_disease = data.data_vars[\"has_heart_disease\"]\n",
    "        y_value = data.data_vars[y]\n",
    "        x_value = data.data_vars[x]\n",
    "        array_x = []\n",
    "        array_y = []\n",
    "        if has_heart_disease == 1:\n",
    "            # Update all of the negative arrays\n",
    "            array_x = negative_x_counts\n",
    "            array_y = negative_y_counts\n",
    "        elif has_heart_disease == 2:\n",
    "            # Update all of the positive arrays\n",
    "            array_x = positive_x_counts\n",
    "            array_y = positive_y_counts\n",
    "            \n",
    "        # Value is not in dictionary yet\n",
    "        # so set the occurence for that value to 1\n",
    "        '''\n",
    "        if value not in array_y.keys():\n",
    "            array_y[y_value] = 1 \n",
    "        # Value is already in dictionary\n",
    "        # so increase the occurence count for that value by 1\n",
    "        else:\n",
    "            current_count_for_value = array_y[y_value]\n",
    "            array_y.update({y_value:current_count_for_value+1})\n",
    "        '''\n",
    "        for key in array_x.keys():\n",
    "            if str(x_value)+\" \" in key:\n",
    "                current_count_for_value = array_x[key]\n",
    "                array_x.update({key:current_count_for_value+1})\n",
    "                \n",
    "        for key in array_y.keys():\n",
    "            if str(y_value) in key:\n",
    "                current_count_for_value = array_y[key]\n",
    "                array_y.update({key:current_count_for_value+1})\n",
    "\n",
    "    '''\n",
    "    y_denom = 0\n",
    "    \n",
    "    for data in presence_heart_array:\n",
    "        y_denom += 1\n",
    "        value = data.data_vars[y]\n",
    "        \n",
    "        # Value is not in dictionary yet\n",
    "        # so set the occurence for that value to 1\n",
    "        if value not in positive_y_counts.keys():\n",
    "            positive_y_counts[value] = 1 \n",
    "        # Value is already in dictionary\n",
    "        # so increase the occurence count for that value by 1\n",
    "        else:\n",
    "            current_count_for_value = positive_y_counts[value]\n",
    "            positive_y_counts.update({value:current_count_for_value+1})\n",
    "            \n",
    "        # Same thing is done for processing x:\n",
    "        x_value = data.data_vars[x]\n",
    "        for key in positive_x_counts.keys():\n",
    "            if str(x_value)+\" \" in key:\n",
    "                current_count_for_value = positive_x_counts[key]\n",
    "                positive_x_counts.update({key:current_count_for_value+1})\n",
    "            \n",
    "    \n",
    "    for data in absence_heart_array:\n",
    "        y_denom += 1\n",
    "        value = data.data_vars[y]\n",
    "        \n",
    "        # Value is not in dictionary yet\n",
    "        # so set the occurence for that value to 1\n",
    "        if value not in negative_y_counts.keys():\n",
    "            negative_y_counts[value] = 1 \n",
    "        # Value is already in dictionary\n",
    "        # so increase the occurence count for that value by 1\n",
    "        else:\n",
    "            current_count_for_value = negative_y_counts[value]\n",
    "            negative_y_counts.update({value:current_count_for_value+1})\n",
    "            \n",
    "        # Same thing is done for processing x:\n",
    "        x_value = data.data_vars[x]\n",
    "        for key in negative_x_counts.keys():\n",
    "            if str(x_value)+\" \" in key:\n",
    "                current_count_for_value = negative_x_counts[key]\n",
    "                negative_x_counts.update({key:current_count_for_value+1})\n",
    "                \n",
    "    '''\n",
    "    '''\n",
    "    if x == \"sex\":\n",
    "        print \"postive_y_counts array\"\n",
    "        print positive_y_counts\n",
    "        print \"negative_y_counts array\"\n",
    "        print negative_y_counts\n",
    "    '''   \n",
    "            \n",
    "    h_of_y = 0\n",
    "    for count in positive_y_counts.values():\n",
    "        '''\n",
    "        if y == \"chest_pain_type\":\n",
    "            print \"positive_y_count: \"+str(count)\n",
    "        '''\n",
    "        \n",
    "        entropy = 0\n",
    "        if count != 0:\n",
    "            p = float(float(count)/float(y_denom))\n",
    "            entropy = -1 * p * (math.log(p, 2))\n",
    "        h_of_y += entropy\n",
    "    for count in negative_y_counts.values():\n",
    "\n",
    "        '''\n",
    "        if y == \"chest_pain_type\":\n",
    "            print \"negative_y_count: \"+str(count)\n",
    "        '''\n",
    "    \n",
    "        entropy = 0\n",
    "        if count != 0:\n",
    "            p = float(float(count)/float(y_denom))\n",
    "            entropy = -1 * p * (math.log(p, 2))\n",
    "        h_of_y += entropy\n",
    "    \n",
    "    h_of_y_given_x = 0\n",
    "    for feature_value in positive_x_counts.keys():\n",
    "        '''\n",
    "        if x == \"sex\":\n",
    "            print \"positive_x_counts: \"+str(positive_x_counts[feature_value])+\" for feature value: \"+str(feature_value)\n",
    "            print \"negative_x_counts: \"+str(negative_x_counts[feature_value])+\" for feature value: \"+str(feature_value)        \n",
    "        '''\n",
    "        entropy_positive = 0\n",
    "        entropy_negative = 0\n",
    "        sum_of_values = positive_x_counts[feature_value] + negative_x_counts[feature_value] \n",
    "        fraction = float(float(sum_of_values)/float(y_denom))\n",
    "        if positive_x_counts[feature_value] != 0:\n",
    "            p_positive = float(float(positive_x_counts[feature_value])/float(sum_of_values)) \n",
    "            entropy_positive = p_positive * (math.log(p_positive, 2))\n",
    "        if negative_x_counts[feature_value] != 0:\n",
    "            p_negative = float(float(negative_x_counts[feature_value])/float(sum_of_values)) \n",
    "            entropy_negative = p_negative * (math.log(p_negative, 2))\n",
    "        \n",
    "        h_of_y_given_x += fraction*(entropy_positive+entropy_negative)\n",
    "    \n",
    "    '''\n",
    "    if x == \"sex\":\n",
    "        print \"h_of_y:\"+str(h_of_y)\n",
    "        print \"h_of_y_given_x:\"+str(h_of_y_given_x)\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    if h_of_y_given_x < 0:\n",
    "        info_gain = h_of_y - (h_of_y_given_x)\n",
    "        return info_gain\n",
    "    else:\n",
    "        info_gain = h_of_y - (-1*h_of_y_given_x)\n",
    "        return info_gain\n",
    "    '''\n",
    "    #print \"h of y: \", h_of_y\n",
    "    #print \"h of y given x: \", h_of_y_given_x\n",
    "    info_gain = h_of_y + h_of_y_given_x\n",
    "    return info_gain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Utility Function used to generate all of the possible splits\n",
    "Credit:  https://stackoverflow.com/questions/19368375/set-partitions-in-python/30134039#30134039"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3810,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def partition(collection):\n",
    "    if len(collection) == 1:\n",
    "        yield [ collection ]\n",
    "        return\n",
    "\n",
    "    first = collection[0]\n",
    "    for smaller in partition(collection[1:]):\n",
    "        # insert `first` in each of the subpartition's subsets\n",
    "        for n, subset in enumerate(smaller):\n",
    "            yield smaller[:n] + [[ first ] + subset]  + smaller[n+1:]\n",
    "        # put `first` in its own subset \n",
    "        yield [ [ first ] ] + smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Determine possible splits\n",
    "These will be used for the information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3811,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get_continuous_binary_split(feature):     \n",
    "#    Gets a feature and splits the data for that feature in all the possible ways to split that\n",
    "#    data into two buckets.  It returns a dictionary where the key is the number it split on and the\n",
    "#    value is a list of two lists.  The first element of the list is all of the elements less than\n",
    "#    the split and the second element of the list is a list of all of the elements greater than or\n",
    "#    equal to the split.\n",
    "#    For example:\n",
    "#           feature = \"age\"\n",
    "#           splits_dict = {50: [[20,40,43...],[50,60,61...]], 60: [[57,45,59...],[60,61,63...]]}\n",
    "def get_continuous_binary_split(feature, considered_data):\n",
    "    # Ensure the function is being called only with continuous features\n",
    "    if features_and_types[feature] != \"continuous\":\n",
    "        raise ValueError('Error in get_continuous_binary_split: input feature is not continuous.')\n",
    "        return\n",
    "    \n",
    "    # Create a list of the possible splits\n",
    "    splits = []\n",
    "    counts_for_feature_values = {}\n",
    "    \n",
    "    print \"The Length of the Considered Data: \",len(considered_data)\n",
    "    \n",
    "    for data in considered_data:\n",
    "        feature_value = float(data.data_vars[feature])\n",
    "        if feature_value not in splits:\n",
    "            splits.append(feature_value)\n",
    "        if feature_value not in counts_for_feature_values:\n",
    "            counts_for_feature_values[feature_value] = 1\n",
    "        else:\n",
    "            current = counts_for_feature_values[feature_value]\n",
    "            counts_for_feature_values[feature_value] = current + 1\n",
    "    \n",
    "    # Process the splits and create the less than list and greater than or equal to list for each\n",
    "    splits_list = []\n",
    "    for split in splits:\n",
    "        lt_split_feature_values = []\n",
    "        gtequal_split_feature_values = []\n",
    "        for data in considered_data:\n",
    "            feature_value = float(data.data_vars[feature])\n",
    "            if feature_value < split:\n",
    "                lt_split_feature_values.append(feature_value)\n",
    "            else:\n",
    "                gtequal_split_feature_values.append(feature_value)\n",
    "        #if len(lt_split_feature_values) != 0:\n",
    "        splits_list.append([lt_split_feature_values, gtequal_split_feature_values])\n",
    "            \n",
    "    # Generate a list of partition dicts\n",
    "    # Each list element will be a list of dictionaries\n",
    "    # The dictionaries will contain the feature_value and the number of times it occurs\n",
    "    # The list of these dictionaries will be split up by partition\n",
    "    partitions_and_values = []\n",
    "    for partition in splits_list:\n",
    "        split_inclusive = []\n",
    "        for l in partition:\n",
    "            set_l = set(l)\n",
    "            count_dict = {}\n",
    "            for feature_value in set_l:\n",
    "                num_of_feature_value_occurences = counts_for_feature_values[feature_value]\n",
    "                count_dict[feature_value] = num_of_feature_value_occurences\n",
    "            split_inclusive.append(count_dict)\n",
    "        partitions_and_values.append(split_inclusive)\n",
    "            \n",
    "    return partitions_and_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3812,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "# get_nominal_split(feature):     \n",
    "#    Gets a nominal feature and splits the data for that feature in all the possible ways to split that\n",
    "#    data into every possible number of buckets.  It returns a dictionary where the key is the number it \n",
    "#    split on and the value is a list of the lists it generated from the split.  \n",
    "\n",
    "def split_list(data, n):\n",
    "    from itertools import combinations, chain\n",
    "    for splits in combinations(range(1, len(data)), n-1):\n",
    "        result = []\n",
    "        prev = None\n",
    "        for split in chain(splits, [None]):\n",
    "            result.append(data[prev:split])\n",
    "            prev = split\n",
    "        yield result\n",
    "        \n",
    "def get_nominal_split(feature, considered_data):\n",
    "    # Ensure the function is being called only with nominal features\n",
    "    if features_and_types[feature] != \"nominal\":\n",
    "        raise ValueError('Error in get_nominal_split: input feature is not nominal.')\n",
    "        return\n",
    "    \n",
    "    #print \"get_nominal_split called with feature: \"+feature\n",
    "    # Determine the unique values for the feature and the counts for the feature_value\n",
    "    splits = {}\n",
    "    split_nums = []\n",
    "    for data in considered_data:\n",
    "        feature_value = float(data.data_vars[feature])\n",
    "        if feature_value not in splits:\n",
    "            split_nums.append(feature_value)\n",
    "            splits[feature_value] = 1\n",
    "        else:\n",
    "            current_num = splits[feature_value]\n",
    "            splits[feature_value] = current_num+1\n",
    "    \n",
    "    #print \"possible values for feature:\"\n",
    "    #print split_nums\n",
    "\n",
    "    # Generate all the possible ways to partition the numbers  \n",
    "    # possible_partitions is a list of tuples where the first element of the tuple\n",
    "    # is the kth possible partition and the second element of the tuple is a list\n",
    "    # of lists of partitions\n",
    "    possible_partitions = []\n",
    "    for p in partition(split_nums):\n",
    "        # Be sure to remove the split where all values are placed in one bucket\n",
    "        if (len(sorted(p)) != 1) or ((len(sorted(p)) == 1) and (len(split_nums) == 1)):\n",
    "            possible_partitions.append(sorted(p))\n",
    "    #print \"possible_partitions:\"\n",
    "    #print possible_partitions\n",
    "                \n",
    "    # Generate a list of partition dicts\n",
    "    # Each list element will be a list of dictionaries\n",
    "    # The dictionaries will contain the feature_value and the number of times it occurs\n",
    "    # The list of these dictionaries will be split up by partition\n",
    "    partitions_and_values = []\n",
    "    for split_lists in possible_partitions:\n",
    "        split_dicts = []\n",
    "        for bucket in split_lists:\n",
    "            bucket_dict = {}\n",
    "            for feature_value in bucket:\n",
    "                num_of_feature_value_occurences = splits[feature_value]\n",
    "                bucket_dict[feature_value] = num_of_feature_value_occurences\n",
    "            split_dicts.append(bucket_dict)\n",
    "        partitions_and_values.append(split_dicts)\n",
    "    return partitions_and_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the best split\n",
    "Checks the information gain for all the possible splits for a certain feature and reports the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3813,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_best_split(y, y_values, feature, considered_data):\n",
    "    # Check if the feature is nominal or continuous and split accordingly\n",
    "    splits = []\n",
    "    if features_and_types[feature] == \"nominal\":\n",
    "        \n",
    "        splits = get_nominal_split(feature, considered_data)\n",
    "    else:\n",
    "        splits = get_continuous_binary_split(feature, considered_data)\n",
    "    \n",
    "    # Create a dictionary where the key is the information gain from a particular\n",
    "    # split and the value is that particular split\n",
    "    info_gains = {}\n",
    "    for split in splits:\n",
    "        info_gain = compute_info_gain(y, y_values, feature, split, considered_data)\n",
    "        info_gains[info_gain] = split\n",
    "        \n",
    "    # Process the dictionary and determine the highest info gain\n",
    "    #print \"info-gains\"\n",
    "    #print \"compute_best_info_gain called with: \"+feature\n",
    "    #print \"considered_data\"\n",
    "    #print considered_data\n",
    "    #print info_gains\n",
    "    max_info_gain = max(info_gains, key=float)\n",
    "    split_yielding_max_info_gain = info_gains[max_info_gain]\n",
    "    \n",
    "    # Print the maximum info gain and corresponding split\n",
    "    #print \"FEATURE: \"+feature\n",
    "    #print \"Greatest Info Gain is: \"+str(max_info_gain)\n",
    "    #print \"from split:\"\n",
    "    #for split in split_yielding_max_info_gain:\n",
    "        #print sorted(set(split))\n",
    "    return max_info_gain, split_yielding_max_info_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Class\n",
    "Set up classes for the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3814,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, splitting_feature, feature_value):\n",
    "        self.splitting_feature = splitting_feature\n",
    "        self.feature_value = feature_value\n",
    "        self.children = []\n",
    "\n",
    "    def add_child(self, obj):\n",
    "        self.children.append(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data\n",
    "Split the data and return a list of arrays of the split data given the feature and the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3815,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_data(feature, splits, considered_data):\n",
    "    #print \"Splitting data for feature: \"+feature\n",
    "    #print \"split is: \"\n",
    "    #print splits\n",
    "    data = []\n",
    "    for split in splits:\n",
    "        data.append([])\n",
    "    separate_buckets = {}\n",
    "    for data in considered_data:\n",
    "        feature_value = float(data.data_vars[feature])\n",
    "        if feature_value not in separate_buckets.keys():\n",
    "            new_list = []\n",
    "            new_list.append(data)\n",
    "            separate_buckets[feature_value] = new_list\n",
    "        else:\n",
    "            current_list = separate_buckets[feature_value]\n",
    "            current_list.append(data)\n",
    "            separate_buckets[feature_value] = current_list\n",
    "        \n",
    "    #splits = [{3.0: 152}, {6.0: 14}, {7.0: 16}]\n",
    "    #OR splits = [{3.0: 152}, [{6.0: 14}, {7.0: 16}]]\n",
    "            \n",
    "    new_data = []\n",
    "    for split in splits:\n",
    "        \n",
    "        print \"Processing split: \", split\n",
    "        \n",
    "        # {0.0: 7}\n",
    "        # {1.0: 4, 2.0: 2, 3.0: 1}\n",
    "        \n",
    "        # Convert it to a list\n",
    "        split_list = []\n",
    "        if len(split) == 1:\n",
    "            \n",
    "            split_list.append(split)\n",
    "        else:\n",
    "            split_list.append(split)\n",
    "            '''\n",
    "            for each_value in split:\n",
    "                print \"Appending each value: \", each_value\n",
    "                split_list.append(each_value)\n",
    "            '''\n",
    "        \n",
    "        # Prepare a list for the data of that split\n",
    "        data_for_split = []\n",
    "        \n",
    "        # Get all the valid feature_values for that particular split\n",
    "        valid_feature_values = []\n",
    "        #print \"SPLIT LIST IN SPLIT DATA: \", split_list\n",
    "        for split_dict in split_list:\n",
    "            for feature_value_key in split_dict.keys():\n",
    "                valid_feature_values.append(feature_value_key)\n",
    "        \n",
    "        # Process the valid feature values, find matching data, and add it to the data for that split\n",
    "        for feature_value in valid_feature_values:\n",
    "            for data_pt in separate_buckets[feature_value]:\n",
    "                data_for_split.append(data_pt)\n",
    "        \n",
    "        # Add the data relevant for a certain split group to the new data array\n",
    "        new_data.append(data_for_split)\n",
    "        if feature == \"number_of_major_vessels\":\n",
    "            print \"Length of data for split: \", len(data_for_split)\n",
    "        \n",
    "    if feature == \"number_of_major_vessels\":\n",
    "        print \"Length of data returned for 1, 2, 3 : \", len(new_data[1])\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Recurse to build the tree\n",
    "Used the information gain function to determine the best splits for each node of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3816,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_tree(y, y_values, data, allowable_features_to_split_on):\n",
    "    info_gains = {}\n",
    "    split_yielding_max_info_gain = []\n",
    "    for feature in allowable_features_to_split_on:\n",
    "        if feature != y:\n",
    "            info_gain, split_yielding_max_info_gain = compute_best_split(y, y_values, feature, data)\n",
    "            info_gains[feature] = [info_gain, split_yielding_max_info_gain]\n",
    "\n",
    "    #print info_gains\n",
    "\n",
    "    # Printing out the information gains for each feature\n",
    "    for feature in info_gains.keys():\n",
    "        print str(info_gains[feature][0])+\" : \"+feature\n",
    "\n",
    "    # Find the maximum in the info_gains returned\n",
    "    max_info_gain = 0\n",
    "    best_split = []\n",
    "    best_feature = \"\"\n",
    "    \n",
    "    # Check if they are all equal to 0\n",
    "    counter = 0\n",
    "    total = 0\n",
    "    for feature in info_gains.keys():\n",
    "        if info_gains[feature][0] == 0:\n",
    "            counter += 1\n",
    "        total += 1\n",
    "    if counter == total:\n",
    "        return\n",
    "    \n",
    "    #print info_gains.keys()\n",
    "    for feature in info_gains.keys():\n",
    "        if info_gains[feature][0] > max_info_gain:\n",
    "            max_info_gain = info_gains[feature][0]\n",
    "            best_split = info_gains[feature][1]\n",
    "            best_feature = feature\n",
    "\n",
    "    #print max_info_gain\n",
    "    #print best_feature\n",
    "    # Split on that feature and optimal split\n",
    "    new_data = split_data(best_feature, best_split, data)\n",
    "\n",
    "    # Define the new y based on this data\n",
    "    y = feature\n",
    "    \n",
    "    #print best_split\n",
    "    return best_feature, new_data, best_split\n",
    "\n",
    "\n",
    "\n",
    "    # Now we have to feed that new data into the tree and branch off into the leaves\n",
    "    # so we have three groups for thal which means it should branch into three leaves:\n",
    "    # one with 3, one with 6, and one with 7.\n",
    "    # And recursively continue branching\n",
    "    \n",
    "    \n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3817,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of the Considered Data:  270\n",
      "The Length of the Considered Data:  270\n",
      "The Length of the Considered Data:  270\n",
      "The Length of the Considered Data:  270\n",
      "The Length of the Considered Data:  270\n",
      "The Length of the Considered Data:  270\n",
      "0.10007075195 : maximum_heart_rate_achieved\n",
      "0.129914712749 : exercise_induced_angina\n",
      "0.0164273041343 : resting_blood_pressure\n",
      "0.0567263200991 : age\n",
      "0.00019260538564 : fasting_blood_sugar\n",
      "0.0267986294625 : serum_cholesterol\n",
      "0.0668955964785 : sex\n",
      "0.119648312513 : oldpeak\n",
      "0.165916419313 : number_of_major_vessels\n",
      "0.111153056361 : slope_peak_exercise\n",
      "0.19220234895 : chest_pain_type\n",
      "0.0241518516218 : resting_electrocardiographic_results\n",
      "0.208555678313 : thal\n",
      "Processing split:  {3.0: 152}\n",
      "Processing split:  {6.0: 14}\n",
      "Processing split:  {7.0: 104}\n",
      "[{3.0: 152}, {6.0: 14}, {7.0: 104}]\n",
      "Length of new data  3\n",
      "Processing the branch for:\n",
      "\t Feature =  thal\n",
      "\t Split   =  {3.0: 152}\n",
      "\t Length of New Data   =  152\n",
      "Length of the data for the next call:  152\n",
      "The Length of the Considered Data:  152\n",
      "The Length of the Considered Data:  152\n",
      "The Length of the Considered Data:  152\n",
      "The Length of the Considered Data:  152\n",
      "The Length of the Considered Data:  152\n",
      "The Length of the Considered Data:  152\n",
      "0.0551869024919 : maximum_heart_rate_achieved\n",
      "0.0572574683166 : exercise_induced_angina\n",
      "0.01992086009 : resting_blood_pressure\n",
      "0.0728039968692 : age\n",
      "0.000191314318807 : fasting_blood_sugar\n",
      "0.037824959709 : serum_cholesterol\n",
      "0.037907882481 : sex\n",
      "0.0716968270753 : oldpeak\n",
      "0.103288842677 : number_of_major_vessels\n",
      "0.0547316508915 : slope_peak_exercise\n",
      "0.126066000702 : chest_pain_type\n",
      "0.0295087096606 : resting_electrocardiographic_results\n",
      "Processing split:  {1.0: 12}\n",
      "Processing split:  {2.0: 33}\n",
      "Processing split:  {3.0: 56}\n",
      "Processing split:  {4.0: 51}\n",
      "*** Next Feature:\n",
      "chest_pain_type\n",
      "*** Next Best Split:\n",
      "[{1.0: 12}, {2.0: 33}, {3.0: 56}, {4.0: 51}]\n",
      "*** Length of Next New Data:\n",
      "4\n",
      "Processing the branch for:\n",
      "\t Feature =  thal\n",
      "\t Split   =  {6.0: 14}\n",
      "\t Length of New Data   =  14\n",
      "Length of the data for the next call:  14\n",
      "The Length of the Considered Data:  14\n",
      "The Length of the Considered Data:  14\n",
      "The Length of the Considered Data:  14\n",
      "The Length of the Considered Data:  14\n",
      "The Length of the Considered Data:  14\n",
      "The Length of the Considered Data:  14\n",
      "0.291691997138 : maximum_heart_rate_achieved\n",
      "0.161256239371 : exercise_induced_angina\n",
      "0.0454316465506 : resting_blood_pressure\n",
      "0.0926512887916 : age\n",
      "0.00742671971925 : fasting_blood_sugar\n",
      "0.394895099856 : serum_cholesterol\n",
      "0.0 : sex\n",
      "0.19811742113 : oldpeak\n",
      "0.689391746743 : number_of_major_vessels\n",
      "0.0993521824195 : slope_peak_exercise\n",
      "0.296980134363 : chest_pain_type\n",
      "0.0112658486486 : resting_electrocardiographic_results\n",
      "Processing split:  {0.0: 7}\n",
      "Length of data for split:  7\n",
      "Processing split:  {1.0: 4, 2.0: 2, 3.0: 1}\n",
      "Length of data for split:  7\n",
      "Length of data returned for 1, 2, 3 :  7\n",
      "*** Next Feature:\n",
      "number_of_major_vessels\n",
      "*** Next Best Split:\n",
      "[{0.0: 7}, {1.0: 4, 2.0: 2, 3.0: 1}]\n",
      "*** Length of Next New Data:\n",
      "2\n",
      "Processing the branch for:\n",
      "\t Feature =  thal\n",
      "\t Split   =  {7.0: 104}\n",
      "\t Length of New Data   =  104\n",
      "Length of the data for the next call:  104\n",
      "The Length of the Considered Data:  104\n",
      "The Length of the Considered Data:  104\n",
      "The Length of the Considered Data:  104\n",
      "The Length of the Considered Data:  104\n",
      "The Length of the Considered Data:  104\n",
      "The Length of the Considered Data:  104\n",
      "0.0895668235164 : maximum_heart_rate_achieved\n",
      "0.0688773470435 : exercise_induced_angina\n",
      "0.0702777168475 : resting_blood_pressure\n",
      "0.0156041768308 : age\n",
      "0.00912801121161 : fasting_blood_sugar\n",
      "0.0293805020029 : serum_cholesterol\n",
      "0.00460350232446 : sex\n",
      "0.111673143519 : oldpeak\n",
      "0.0954723538634 : number_of_major_vessels\n",
      "0.0757792363588 : slope_peak_exercise\n",
      "0.159124482731 : chest_pain_type\n",
      "0.0493861605936 : resting_electrocardiographic_results\n",
      "Processing split:  {1.0: 6}\n",
      "Processing split:  {2.0: 7}\n",
      "Processing split:  {3.0: 21}\n",
      "Processing split:  {4.0: 70}\n",
      "*** Next Feature:\n",
      "chest_pain_type\n",
      "*** Next Best Split:\n",
      "[{1.0: 6}, {2.0: 7}, {3.0: 21}, {4.0: 70}]\n",
      "*** Length of Next New Data:\n",
      "4\n",
      "BEFORE DELETING ANYTHING\n",
      "chest_pain_type\n",
      "number_of_major_vessels\n",
      "chest_pain_type\n",
      "ASSIGNING NEW LEVELS\n",
      "chest_pain_type\n",
      "number_of_major_vessels\n",
      "chest_pain_type\n",
      "Processing the branch for:\n",
      "\t Feature =  chest_pain_type\n",
      "\t Split   =  {1.0: 12}\n",
      "\t Length of New Data   =  12\n",
      "Length of the data for the next call:  12\n",
      "The Length of the Considered Data:  12\n",
      "The Length of the Considered Data:  12\n",
      "The Length of the Considered Data:  12\n",
      "The Length of the Considered Data:  12\n",
      "The Length of the Considered Data:  12\n",
      "The Length of the Considered Data:  12\n",
      "0.0363730992219 : maximum_heart_rate_achieved\n",
      "0.0768690417669 : exercise_induced_angina\n",
      "0.122556248918 : resting_blood_pressure\n",
      "0.174988789176 : age\n",
      "0.00855078606405 : fasting_blood_sugar\n",
      "0.204260414864 : serum_cholesterol\n",
      "0.174988789176 : sex\n",
      "0.043004712053 : oldpeak\n",
      "0.0271189966077 : number_of_major_vessels\n",
      "0.132802336953 : slope_peak_exercise\n",
      "0.0 : resting_electrocardiographic_results\n",
      "0.0 : thal\n",
      "Processing split:  {226.0: 1, 234.0: 2, 239.0: 1, 240.0: 1, 211.0: 1, 244.0: 1, 213.0: 1, 182.0: 1}\n",
      "Processing split:  {273.0: 1, 282.0: 1, 283.0: 1}\n",
      "*** Next Feature:\n",
      "serum_cholesterol\n",
      "*** Next Best Split:\n",
      "[{226.0: 1, 234.0: 2, 239.0: 1, 240.0: 1, 211.0: 1, 244.0: 1, 213.0: 1, 182.0: 1}, {273.0: 1, 282.0: 1, 283.0: 1}]\n",
      "*** Length of Next New Data:\n",
      "2\n",
      "Processing the branch for:\n",
      "\t Feature =  chest_pain_type\n",
      "\t Split   =  {2.0: 33}\n",
      "\t Length of New Data   =  33\n",
      "Length of the data for the next call:  33\n",
      "The Length of the Considered Data:  33\n",
      "The Length of the Considered Data:  33\n",
      "The Length of the Considered Data:  33\n",
      "The Length of the Considered Data:  33\n",
      "The Length of the Considered Data:  33\n",
      "The Length of the Considered Data:  33\n",
      "0.101833258327 : maximum_heart_rate_achieved\n",
      "0.00861031109676 : exercise_induced_angina\n",
      "0.0246744212182 : resting_blood_pressure\n",
      "0.13317258549 : age\n",
      "0.0413423266034 : fasting_blood_sugar\n",
      "0.121426659341 : serum_cholesterol\n",
      "4.27560517526e-05 : sex\n",
      "0.135303210268 : oldpeak\n",
      "0.0546898122763 : number_of_major_vessels\n",
      "0.0190358466503 : slope_peak_exercise\n",
      "0.0788333762631 : resting_electrocardiographic_results\n",
      "0.0 : thal\n",
      "Processing split:  {0.0: 19, 0.4: 1, 0.2: 3, 0.8: 2, 0.6: 2, 1.1: 1, 1.3: 1, 0.7: 1, 1.4: 2}\n",
      "Processing split:  {1.8: 1}\n",
      "*** Next Feature:\n",
      "oldpeak\n",
      "*** Next Best Split:\n",
      "[{0.0: 19, 0.4: 1, 0.2: 3, 0.8: 2, 0.6: 2, 1.1: 1, 1.3: 1, 0.7: 1, 1.4: 2}, {1.8: 1}]\n",
      "*** Length of Next New Data:\n",
      "2\n",
      "Processing the branch for:\n",
      "\t Feature =  chest_pain_type\n",
      "\t Split   =  {3.0: 56}\n",
      "\t Length of New Data   =  56\n",
      "Length of the data for the next call:  56\n",
      "The Length of the Considered Data:  56\n",
      "The Length of the Considered Data:  56\n",
      "The Length of the Considered Data:  56\n",
      "The Length of the Considered Data:  56\n",
      "The Length of the Considered Data:  56\n",
      "The Length of the Considered Data:  56\n",
      "0.100780647255 : maximum_heart_rate_achieved\n",
      "0.0126489416564 : exercise_induced_angina\n",
      "0.0267610504036 : resting_blood_pressure\n",
      "0.0458208034332 : age\n",
      "0.0296771009882 : fasting_blood_sugar\n",
      "0.0310254837884 : serum_cholesterol\n",
      "0.100780647255 : sex\n",
      "0.0878851449487 : oldpeak\n",
      "0.0310254837884 : number_of_major_vessels\n",
      "0.0141026731849 : slope_peak_exercise\n",
      "0.00252892803837 : resting_electrocardiographic_results\n",
      "0.0 : thal\n",
      "Processing split:  {96.0: 1, 130.0: 1, 115.0: 1, 155.0: 1, 137.0: 1, 139.0: 1, 126.0: 1, 142.0: 1, 143.0: 1, 147.0: 1, 116.0: 1, 149.0: 2, 150.0: 1, 151.0: 1, 152.0: 3, 148.0: 1, 123.0: 1, 156.0: 1, 157.0: 3, 158.0: 3}\n",
      "Processing split:  {160.0: 2, 162.0: 1, 163.0: 2, 165.0: 2, 166.0: 1, 167.0: 1, 168.0: 1, 169.0: 2, 170.0: 2, 172.0: 4, 173.0: 2, 174.0: 1, 175.0: 2, 179.0: 3, 180.0: 1, 182.0: 1, 187.0: 1}\n",
      "*** Next Feature:\n",
      "maximum_heart_rate_achieved\n",
      "*** Next Best Split:\n",
      "[{96.0: 1, 130.0: 1, 115.0: 1, 155.0: 1, 137.0: 1, 139.0: 1, 126.0: 1, 142.0: 1, 143.0: 1, 147.0: 1, 116.0: 1, 149.0: 2, 150.0: 1, 151.0: 1, 152.0: 3, 148.0: 1, 123.0: 1, 156.0: 1, 157.0: 3, 158.0: 3}, {160.0: 2, 162.0: 1, 163.0: 2, 165.0: 2, 166.0: 1, 167.0: 1, 168.0: 1, 169.0: 2, 170.0: 2, 172.0: 4, 173.0: 2, 174.0: 1, 175.0: 2, 179.0: 3, 180.0: 1, 182.0: 1, 187.0: 1}]\n",
      "*** Length of Next New Data:\n",
      "2\n",
      "Processing the branch for:\n",
      "\t Feature =  chest_pain_type\n",
      "\t Split   =  {4.0: 51}\n",
      "\t Length of New Data   =  51\n",
      "Length of the data for the next call:  51\n",
      "The Length of the Considered Data:  51\n",
      "The Length of the Considered Data:  51\n",
      "The Length of the Considered Data:  51\n",
      "The Length of the Considered Data:  51\n",
      "The Length of the Considered Data:  51\n",
      "The Length of the Considered Data:  51\n",
      "0.206387096729 : maximum_heart_rate_achieved\n",
      "0.119151895098 : exercise_induced_angina\n",
      "0.0400965587138 : resting_blood_pressure\n",
      "0.124654017215 : age\n",
      "0.0464637444695 : fasting_blood_sugar\n",
      "0.0435652983925 : serum_cholesterol\n",
      "0.0486660246663 : sex\n",
      "0.0960140034866 : oldpeak\n",
      "0.323041522863 : number_of_major_vessels\n",
      "0.0981449870867 : slope_peak_exercise\n",
      "0.0504700300096 : resting_electrocardiographic_results\n",
      "0.0 : thal\n",
      "Processing split:  {0.0: 31}\n",
      "Length of data for split:  31\n",
      "Processing split:  {1.0: 10, 2.0: 6, 3.0: 4}\n",
      "Length of data for split:  20\n",
      "Length of data returned for 1, 2, 3 :  20\n",
      "*** Next Feature:\n",
      "number_of_major_vessels\n",
      "*** Next Best Split:\n",
      "[{0.0: 31}, {1.0: 10, 2.0: 6, 3.0: 4}]\n",
      "*** Length of Next New Data:\n",
      "2\n",
      "Processing the branch for:\n",
      "\t Feature =  number_of_major_vessels\n",
      "\t Split   =  {0.0: 7}\n",
      "\t Length of New Data   =  7\n",
      "Length of the data for the next call:  7\n",
      "The Length of the Considered Data:  7\n",
      "The Length of the Considered Data:  7\n",
      "The Length of the Considered Data:  7\n",
      "The Length of the Considered Data:  7\n",
      "The Length of the Considered Data:  7\n",
      "0.591672778582 : maximum_heart_rate_achieved\n",
      "0.305958492868 : exercise_induced_angina\n",
      "0.128085278891 : resting_blood_pressure\n",
      "0.305958492868 : age\n",
      "0.0345107028837 : fasting_blood_sugar\n",
      "0.591672778582 : serum_cholesterol\n",
      "0.0 : sex\n",
      "0.19811742113 : oldpeak\n",
      "0.0760098536628 : slope_peak_exercise\n",
      "0.128085278891 : chest_pain_type\n",
      "0.128085278891 : resting_electrocardiographic_results\n",
      "0.0 : thal\n",
      "Processing split:  {125.0: 1}\n",
      "Processing split:  {132.0: 1, 138.0: 1, 148.0: 1, 190.0: 1, 150.0: 1, 126.0: 1}\n",
      "*** Next Feature:\n",
      "maximum_heart_rate_achieved\n",
      "*** Next Best Split:\n",
      "[{125.0: 1}, {132.0: 1, 138.0: 1, 148.0: 1, 190.0: 1, 150.0: 1, 126.0: 1}]\n",
      "*** Length of Next New Data:\n",
      "2\n",
      "Processing the branch for:\n",
      "\t Feature =  number_of_major_vessels\n",
      "\t Split   =  {1.0: 4, 2.0: 2, 3.0: 1}\n",
      "\t Length of New Data   =  7\n",
      "Length of the data for the next call:  7\n",
      "The Length of the Considered Data:  7\n",
      "The Length of the Considered Data:  7\n",
      "The Length of the Considered Data:  7\n",
      "The Length of the Considered Data:  7\n",
      "The Length of the Considered Data:  7\n",
      "1.37878349349 : maximum_heart_rate_achieved\n",
      "1.37878349349 : exercise_induced_angina\n",
      "1.37878349349 : resting_blood_pressure\n",
      "1.37878349349 : age\n",
      "1.37878349349 : fasting_blood_sugar\n",
      "1.37878349349 : serum_cholesterol\n",
      "1.37878349349 : sex\n",
      "1.37878349349 : oldpeak\n",
      "1.37878349349 : slope_peak_exercise\n",
      "1.37878349349 : chest_pain_type\n",
      "1.37878349349 : resting_electrocardiographic_results\n",
      "1.37878349349 : thal\n",
      "Processing split:  {112.0: 1, 105.0: 1}\n",
      "Processing split:  {120.0: 1, 132.0: 1, 142.0: 1, 134.0: 1, 158.0: 1}\n",
      "*** Next Feature:\n",
      "maximum_heart_rate_achieved\n",
      "*** Next Best Split:\n",
      "[{112.0: 1, 105.0: 1}, {120.0: 1, 132.0: 1, 142.0: 1, 134.0: 1, 158.0: 1}]\n",
      "*** Length of Next New Data:\n",
      "2\n",
      "Processing the branch for:\n",
      "\t Feature =  chest_pain_type\n",
      "\t Split   =  {1.0: 6}\n",
      "\t Length of New Data   =  6\n",
      "Length of the data for the next call:  6\n",
      "The Length of the Considered Data:  6\n",
      "The Length of the Considered Data:  6\n",
      "The Length of the Considered Data:  6\n",
      "The Length of the Considered Data:  6\n",
      "The Length of the Considered Data:  6\n",
      "The Length of the Considered Data:  6\n",
      "0.316689088315 : maximum_heart_rate_achieved\n",
      "0.0441104177484 : exercise_induced_angina\n",
      "0.109170338676 : resting_blood_pressure\n",
      "0.316689088315 : age\n",
      "0.109170338676 : fasting_blood_sugar\n",
      "0.251629167388 : serum_cholesterol\n",
      "0.0 : sex\n",
      "0.316689088315 : oldpeak\n",
      "0.0 : number_of_major_vessels\n",
      "0.251629167388 : slope_peak_exercise\n",
      "0.0 : resting_electrocardiographic_results\n",
      "0.0 : thal\n",
      "Processing split:  {145.0: 1, 178.0: 2, 162.0: 1, 159.0: 1}\n",
      "Processing split:  {182.0: 1}\n",
      "*** Next Feature:\n",
      "maximum_heart_rate_achieved\n",
      "*** Next Best Split:\n",
      "[{145.0: 1, 178.0: 2, 162.0: 1, 159.0: 1}, {182.0: 1}]\n",
      "*** Length of Next New Data:\n",
      "2\n",
      "Processing the branch for:\n",
      "\t Feature =  chest_pain_type\n",
      "\t Split   =  {2.0: 7}\n",
      "\t Length of New Data   =  7\n",
      "Length of the data for the next call:  7\n",
      "The Length of the Considered Data:  7\n",
      "The Length of the Considered Data:  7\n",
      "The Length of the Considered Data:  7\n",
      "The Length of the Considered Data:  7\n",
      "The Length of the Considered Data:  7\n",
      "The Length of the Considered Data:  7\n",
      "0.291691997138 : maximum_heart_rate_achieved\n",
      "0.0 : exercise_induced_angina\n",
      "0.469565211115 : resting_blood_pressure\n",
      "0.469565211115 : age\n",
      "0.19811742113 : fasting_blood_sugar\n",
      "0.19811742113 : serum_cholesterol\n",
      "0.0 : sex\n",
      "0.521640636343 : oldpeak\n",
      "0.291691997138 : number_of_major_vessels\n",
      "0.291691997138 : slope_peak_exercise\n",
      "0.291691997138 : resting_electrocardiographic_results\n",
      "0.0 : thal\n",
      "Processing split:  {0.0: 4}\n",
      "Processing split:  {1.4: 1, 0.3: 1, 1.0: 1}\n",
      "*** Next Feature:\n",
      "oldpeak\n",
      "*** Next Best Split:\n",
      "[{0.0: 4}, {1.4: 1, 0.3: 1, 1.0: 1}]\n",
      "*** Length of Next New Data:\n",
      "2\n",
      "Processing the branch for:\n",
      "\t Feature =  chest_pain_type\n",
      "\t Split   =  {3.0: 21}\n",
      "\t Length of New Data   =  21\n",
      "Length of the data for the next call:  21\n",
      "The Length of the Considered Data:  21\n",
      "The Length of the Considered Data:  21\n",
      "The Length of the Considered Data:  21\n",
      "The Length of the Considered Data:  21\n",
      "The Length of the Considered Data:  21\n",
      "The Length of the Considered Data:  21\n",
      "0.172010445615 : maximum_heart_rate_achieved\n",
      "0.013890172361 : exercise_induced_angina\n",
      "0.0954104919474 : resting_blood_pressure\n",
      "0.109941055907 : age\n",
      "0.0363324446837 : fasting_blood_sugar\n",
      "0.0954104919474 : serum_cholesterol\n",
      "0.0100356963003 : sex\n",
      "0.315667876377 : oldpeak\n",
      "0.146106978916 : number_of_major_vessels\n",
      "0.249882683407 : slope_peak_exercise\n",
      "0.0136917860053 : resting_electrocardiographic_results\n",
      "0.0 : thal\n",
      "Processing split:  {0.0: 1, 1.0: 1, 0.2: 2, 1.6: 3, 0.4: 2, 0.6: 2, 1.2: 1, 0.8: 1, 1.8: 2, 0.5: 1}\n",
      "Processing split:  {2.5: 1, 2.0: 2, 2.9: 1, 3.2: 1}\n",
      "*** Next Feature:\n",
      "oldpeak\n",
      "*** Next Best Split:\n",
      "[{0.0: 1, 1.0: 1, 0.2: 2, 1.6: 3, 0.4: 2, 0.6: 2, 1.2: 1, 0.8: 1, 1.8: 2, 0.5: 1}, {2.5: 1, 2.0: 2, 2.9: 1, 3.2: 1}]\n",
      "*** Length of Next New Data:\n",
      "2\n",
      "Processing the branch for:\n",
      "\t Feature =  chest_pain_type\n",
      "\t Split   =  {4.0: 70}\n",
      "\t Length of New Data   =  70\n",
      "Length of the data for the next call:  70\n",
      "The Length of the Considered Data:  70\n",
      "The Length of the Considered Data:  70\n",
      "The Length of the Considered Data:  70\n",
      "The Length of the Considered Data:  70\n",
      "The Length of the Considered Data:  70\n",
      "The Length of the Considered Data:  70\n",
      "0.0257599564416 : maximum_heart_rate_achieved\n",
      "0.0175653629631 : exercise_induced_angina\n",
      "0.045595714245 : resting_blood_pressure\n",
      "0.018052379906 : age\n",
      "0.000126341127727 : fasting_blood_sugar\n",
      "0.0716565616269 : serum_cholesterol\n",
      "0.0235360622999 : sex\n",
      "0.221089625496 : oldpeak\n",
      "0.0389843681291 : number_of_major_vessels\n",
      "0.0657208312303 : slope_peak_exercise\n",
      "0.0592334950454 : resting_electrocardiographic_results\n",
      "0.0 : thal\n",
      "Processing split:  {0.5: 2, 0.0: 11, 0.2: 2, 0.4: 1, 0.1: 2}\n",
      "Processing split:  {2.5: 1, 1.0: 5, 2.0: 3, 3.0: 2, 4.0: 2, 1.9: 2, 4.2: 1, 2.8: 4, 5.6: 1, 3.4: 1, 3.6: 1, 0.9: 1, 6.2: 1, 1.2: 6, 2.4: 1, 0.8: 3, 1.4: 4, 2.2: 2, 1.6: 3, 1.8: 3, 2.6: 4, 3.1: 1}\n",
      "*** Next Feature:\n",
      "oldpeak\n",
      "*** Next Best Split:\n",
      "[{0.5: 2, 0.0: 11, 0.2: 2, 0.4: 1, 0.1: 2}, {2.5: 1, 1.0: 5, 2.0: 3, 3.0: 2, 4.0: 2, 1.9: 2, 4.2: 1, 2.8: 4, 5.6: 1, 3.4: 1, 3.6: 1, 0.9: 1, 6.2: 1, 1.2: 6, 2.4: 1, 0.8: 3, 1.4: 4, 2.2: 2, 1.6: 3, 1.8: 3, 2.6: 4, 3.1: 1}]\n",
      "*** Length of Next New Data:\n",
      "2\n",
      "BEFORE DELETING ANYTHING\n",
      "chest_pain_type\n",
      "number_of_major_vessels\n",
      "chest_pain_type\n",
      "serum_cholesterol\n",
      "oldpeak\n",
      "maximum_heart_rate_achieved\n",
      "number_of_major_vessels\n",
      "maximum_heart_rate_achieved\n",
      "maximum_heart_rate_achieved\n",
      "maximum_heart_rate_achieved\n",
      "oldpeak\n",
      "oldpeak\n",
      "oldpeak\n",
      "i =  0\n",
      "Removing  chest_pain_type  from the list.\n",
      "i =  1\n",
      "Removing  chest_pain_type  from the list.\n",
      "i =  2\n",
      "Removing  oldpeak  from the list.\n",
      "ASSIGNING NEW LEVELS\n",
      "number_of_major_vessels\n",
      "serum_cholesterol\n",
      "maximum_heart_rate_achieved\n",
      "number_of_major_vessels\n",
      "maximum_heart_rate_achieved\n",
      "maximum_heart_rate_achieved\n",
      "maximum_heart_rate_achieved\n",
      "oldpeak\n",
      "oldpeak\n",
      "oldpeak\n",
      "Processing the branch for:\n",
      "\t Feature =  number_of_major_vessels\n",
      "\t Split   =  {0.0: 7}\n",
      "\t Length of New Data   =  7\n",
      "Length of the data for the next call:  7\n",
      "The Length of the Considered Data:  7\n",
      "The Length of the Considered Data:  7\n",
      "The Length of the Considered Data:  7\n",
      "The Length of the Considered Data:  7\n",
      "The Length of the Considered Data:  7\n",
      "0.591672778582 : maximum_heart_rate_achieved\n",
      "0.305958492868 : exercise_induced_angina\n",
      "0.128085278891 : resting_blood_pressure\n",
      "0.305958492868 : age\n",
      "0.0345107028837 : fasting_blood_sugar\n",
      "0.591672778582 : serum_cholesterol\n",
      "0.0 : sex\n",
      "0.19811742113 : oldpeak\n",
      "0.0760098536628 : slope_peak_exercise\n",
      "0.128085278891 : chest_pain_type\n",
      "0.128085278891 : resting_electrocardiographic_results\n",
      "0.0 : thal\n",
      "Processing split:  {125.0: 1}\n",
      "Processing split:  {132.0: 1, 138.0: 1, 148.0: 1, 190.0: 1, 150.0: 1, 126.0: 1}\n",
      "*** Next Feature:\n",
      "maximum_heart_rate_achieved\n",
      "*** Next Best Split:\n",
      "[{125.0: 1}, {132.0: 1, 138.0: 1, 148.0: 1, 190.0: 1, 150.0: 1, 126.0: 1}]\n",
      "*** Length of Next New Data:\n",
      "2\n",
      "Processing the branch for:\n",
      "\t Feature =  number_of_major_vessels\n",
      "\t Split   =  {1.0: 4, 2.0: 2, 3.0: 1}\n",
      "\t Length of New Data   =  7\n",
      "Length of the data for the next call:  7\n",
      "The Length of the Considered Data:  7\n",
      "The Length of the Considered Data:  7\n",
      "The Length of the Considered Data:  7\n",
      "The Length of the Considered Data:  7\n",
      "The Length of the Considered Data:  7\n",
      "1.37878349349 : maximum_heart_rate_achieved\n",
      "1.37878349349 : exercise_induced_angina\n",
      "1.37878349349 : resting_blood_pressure\n",
      "1.37878349349 : age\n",
      "1.37878349349 : fasting_blood_sugar\n",
      "1.37878349349 : serum_cholesterol\n",
      "1.37878349349 : sex\n",
      "1.37878349349 : oldpeak\n",
      "1.37878349349 : slope_peak_exercise\n",
      "1.37878349349 : chest_pain_type\n",
      "1.37878349349 : resting_electrocardiographic_results\n",
      "1.37878349349 : thal\n",
      "Processing split:  {112.0: 1, 105.0: 1}\n",
      "Processing split:  {120.0: 1, 132.0: 1, 142.0: 1, 134.0: 1, 158.0: 1}\n",
      "*** Next Feature:\n",
      "maximum_heart_rate_achieved\n",
      "*** Next Best Split:\n",
      "[{112.0: 1, 105.0: 1}, {120.0: 1, 132.0: 1, 142.0: 1, 134.0: 1, 158.0: 1}]\n",
      "*** Length of Next New Data:\n",
      "2\n",
      "Processing the branch for:\n",
      "\t Feature =  serum_cholesterol\n",
      "\t Split   =  {226.0: 1, 234.0: 2, 239.0: 1, 240.0: 1, 211.0: 1, 244.0: 1, 213.0: 1, 182.0: 1}\n",
      "\t Length of New Data   =  9\n",
      "Length of the data for the next call:  9\n",
      "The Length of the Considered Data:  9\n",
      "The Length of the Considered Data:  9\n",
      "The Length of the Considered Data:  9\n",
      "The Length of the Considered Data:  9\n",
      "The Length of the Considered Data:  9\n",
      "2.76885383762 : maximum_heart_rate_achieved\n",
      "2.70973506254 : exercise_induced_angina\n",
      "2.80935694613 : resting_blood_pressure\n",
      "2.76885383762 : age\n",
      "2.68675660749 : fasting_blood_sugar\n",
      "2.73657672034 : sex\n",
      "2.94770277922 : oldpeak\n",
      "2.86382639009 : number_of_major_vessels\n",
      "2.86382639009 : slope_peak_exercise\n",
      "2.66666666667 : chest_pain_type\n",
      "2.80935694613 : resting_electrocardiographic_results\n",
      "2.66666666667 : thal\n",
      "Processing split:  {0.0: 1, 0.1: 1, 0.8: 1, 1.8: 2, 0.9: 1, 1.4: 1}\n",
      "Processing split:  {2.6: 2}\n",
      "*** Next Feature:\n",
      "oldpeak\n",
      "*** Next Best Split:\n",
      "[{0.0: 1, 0.1: 1, 0.8: 1, 1.8: 2, 0.9: 1, 1.4: 1}, {2.6: 2}]\n",
      "*** Length of Next New Data:\n",
      "2\n",
      "Processing the branch for:\n",
      "\t Feature =  serum_cholesterol\n",
      "\t Split   =  {273.0: 1, 282.0: 1, 283.0: 1}\n",
      "\t Length of New Data   =  3\n",
      "Length of the data for the next call:  3\n",
      "The Length of the Considered Data:  3\n",
      "The Length of the Considered Data:  3\n",
      "The Length of the Considered Data:  3\n",
      "The Length of the Considered Data:  3\n",
      "The Length of the Considered Data:  3\n",
      "0.918295834054 : maximum_heart_rate_achieved\n",
      "0.666666666667 : exercise_induced_angina\n",
      "0.918295834054 : resting_blood_pressure\n",
      "1.58496250072 : age\n",
      "0.918295834054 : fasting_blood_sugar\n",
      "1.58496250072 : sex\n",
      "0.918295834054 : oldpeak\n",
      "0.918295834054 : number_of_major_vessels\n",
      "0.918295834054 : slope_peak_exercise\n",
      "0.666666666667 : chest_pain_type\n",
      "0.666666666667 : resting_electrocardiographic_results\n",
      "0.666666666667 : thal\n",
      "Processing split:  {58.0: 1}\n",
      "Processing split:  {65.0: 1, 59.0: 1}\n",
      "*** Next Feature:\n",
      "age\n",
      "*** Next Best Split:\n",
      "[{58.0: 1}, {65.0: 1, 59.0: 1}]\n",
      "*** Length of Next New Data:\n",
      "2\n",
      "Processing the branch for:\n",
      "\t Feature =  maximum_heart_rate_achieved\n",
      "\t Split   =  {96.0: 1, 130.0: 1, 115.0: 1, 155.0: 1, 137.0: 1, 139.0: 1, 126.0: 1, 142.0: 1, 143.0: 1, 147.0: 1, 116.0: 1, 149.0: 2, 150.0: 1, 151.0: 1, 152.0: 3, 148.0: 1, 123.0: 1, 156.0: 1, 157.0: 3, 158.0: 3}\n",
      "\t Length of New Data   =  27\n",
      "Length of the data for the next call:  27\n",
      "The Length of the Considered Data:  27\n",
      "The Length of the Considered Data:  27\n",
      "The Length of the Considered Data:  27\n",
      "The Length of the Considered Data:  27\n",
      "The Length of the Considered Data:  27\n",
      "3.68810639571 : exercise_induced_angina\n",
      "3.70113543322 : resting_blood_pressure\n",
      "3.75372467527 : age\n",
      "3.74066850564 : fasting_blood_sugar\n",
      "3.76011622202 : serum_cholesterol\n",
      "3.89374071092 : sex\n",
      "3.86640938608 : oldpeak\n",
      "3.76011622202 : number_of_major_vessels\n",
      "3.68925327169 : slope_peak_exercise\n",
      "3.66526846601 : chest_pain_type\n",
      "3.67898045907 : resting_electrocardiographic_results\n",
      "3.66526846601 : thal\n",
      "Processing split:  {0.0: 14}\n",
      "Processing split:  {1.0: 13}\n",
      "*** Next Feature:\n",
      "sex\n",
      "*** Next Best Split:\n",
      "[{0.0: 14}, {1.0: 13}]\n",
      "*** Length of Next New Data:\n",
      "2\n",
      "Processing the branch for:\n",
      "\t Feature =  maximum_heart_rate_achieved\n",
      "\t Split   =  {160.0: 2, 162.0: 1, 163.0: 2, 165.0: 2, 166.0: 1, 167.0: 1, 168.0: 1, 169.0: 2, 170.0: 2, 172.0: 4, 173.0: 2, 174.0: 1, 175.0: 2, 179.0: 3, 180.0: 1, 182.0: 1, 187.0: 1}\n",
      "\t Length of New Data   =  29\n",
      "Length of the data for the next call:  29\n",
      "The Length of the Considered Data:  29\n",
      "The Length of the Considered Data:  29\n",
      "The Length of the Considered Data:  29\n",
      "The Length of the Considered Data:  29\n",
      "The Length of the Considered Data:  29\n",
      "3.93539866747 : exercise_induced_angina\n",
      "3.93539866747 : resting_blood_pressure\n",
      "3.93539866747 : age\n",
      "3.93539866747 : fasting_blood_sugar\n",
      "3.93539866747 : serum_cholesterol\n",
      "3.93539866747 : sex\n",
      "3.93539866747 : oldpeak\n",
      "3.93539866747 : number_of_major_vessels\n",
      "3.93539866747 : slope_peak_exercise\n",
      "3.93539866747 : chest_pain_type\n",
      "3.93539866747 : resting_electrocardiographic_results\n",
      "3.93539866747 : thal\n",
      "Processing split:  {0.0: 26}\n",
      "Processing split:  {1.0: 3}\n",
      "*** Next Feature:\n",
      "exercise_induced_angina\n",
      "*** Next Best Split:\n",
      "[{0.0: 26}, {1.0: 3}]\n",
      "*** Length of Next New Data:\n",
      "2\n",
      "Processing the branch for:\n",
      "\t Feature =  number_of_major_vessels\n",
      "\t Split   =  {0.0: 31}\n",
      "\t Length of New Data   =  31\n",
      "Length of the data for the next call:  31\n",
      "The Length of the Considered Data:  31\n",
      "The Length of the Considered Data:  31\n",
      "The Length of the Considered Data:  31\n",
      "The Length of the Considered Data:  31\n",
      "The Length of the Considered Data:  31\n",
      "0.0344276778832 : exercise_induced_angina\n",
      "0.206762939159 : resting_blood_pressure\n",
      "0.263893934478 : age\n",
      "0.0 : fasting_blood_sugar\n",
      "0.108909202181 : serum_cholesterol\n",
      "0.0390351708861 : sex\n",
      "0.0797817168983 : oldpeak\n",
      "0.0324744692335 : slope_peak_exercise\n",
      "0.0 : chest_pain_type\n",
      "0.0897593929184 : resting_electrocardiographic_results\n",
      "0.0807493834284 : maximum_heart_rate_achieved\n",
      "0.0 : thal\n",
      "Processing split:  {35.0: 1, 42.0: 2, 43.0: 2, 45.0: 3, 46.0: 1, 47.0: 1, 48.0: 1, 49.0: 1, 50.0: 1, 51.0: 1, 53.0: 2, 54.0: 1}\n",
      "Processing split:  {64.0: 1, 66.0: 1, 67.0: 1, 71.0: 1, 55.0: 1, 57.0: 1, 58.0: 1, 59.0: 2, 60.0: 1, 61.0: 1, 62.0: 3}\n",
      "*** Next Feature:\n",
      "age\n",
      "*** Next Best Split:\n",
      "[{35.0: 1, 42.0: 2, 43.0: 2, 45.0: 3, 46.0: 1, 47.0: 1, 48.0: 1, 49.0: 1, 50.0: 1, 51.0: 1, 53.0: 2, 54.0: 1}, {64.0: 1, 66.0: 1, 67.0: 1, 71.0: 1, 55.0: 1, 57.0: 1, 58.0: 1, 59.0: 2, 60.0: 1, 61.0: 1, 62.0: 3}]\n",
      "*** Length of Next New Data:\n",
      "2\n",
      "Processing the branch for:\n",
      "\t Feature =  number_of_major_vessels\n",
      "\t Split   =  {1.0: 10, 2.0: 6, 3.0: 4}\n",
      "\t Length of New Data   =  20\n",
      "Length of the data for the next call:  20\n",
      "The Length of the Considered Data:  20\n",
      "The Length of the Considered Data:  20\n",
      "The Length of the Considered Data:  20\n",
      "The Length of the Considered Data:  20\n",
      "The Length of the Considered Data:  20\n",
      "1.58222871891 : exercise_induced_angina\n",
      "1.44252934294 : resting_blood_pressure\n",
      "1.48888445 : age\n",
      "1.41044166475 : fasting_blood_sugar\n",
      "1.52562979094 : serum_cholesterol\n",
      "1.69546184424 : sex\n",
      "1.45401577307 : oldpeak\n",
      "1.45164630243 : slope_peak_exercise\n",
      "1.38562153952 : chest_pain_type\n",
      "1.48770469436 : resting_electrocardiographic_results\n",
      "1.48888445 : maximum_heart_rate_achieved\n",
      "1.38562153952 : thal\n",
      "Processing split:  {0.0: 6}\n",
      "Processing split:  {1.0: 14}\n",
      "*** Next Feature:\n",
      "sex\n",
      "*** Next Best Split:\n",
      "[{0.0: 6}, {1.0: 14}]\n",
      "*** Length of Next New Data:\n",
      "2\n",
      "Processing the branch for:\n",
      "\t Feature =  maximum_heart_rate_achieved\n",
      "\t Split   =  {125.0: 1}\n",
      "\t Length of New Data   =  1\n",
      "Length of the data for the next call:  1\n",
      "The Length of the Considered Data:  1\n",
      "The Length of the Considered Data:  1\n",
      "The Length of the Considered Data:  1\n",
      "The Length of the Considered Data:  1\n",
      "The Length of the Considered Data:  1\n",
      "0.0 : exercise_induced_angina\n",
      "0.0 : resting_blood_pressure\n",
      "0.0 : age\n",
      "0.0 : fasting_blood_sugar\n",
      "0.0 : serum_cholesterol\n",
      "0.0 : sex\n",
      "0.0 : oldpeak\n",
      "0.0 : number_of_major_vessels\n",
      "0.0 : slope_peak_exercise\n",
      "0.0 : chest_pain_type\n",
      "0.0 : resting_electrocardiographic_results\n",
      "0.0 : thal\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3817-8bc86bc53484>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m                 '''\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mnext_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_new_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_best_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossible_y_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_for_next_call\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowable_features_to_split_on\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0;34m\"*** Next Feature:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "y = \"has_heart_disease\"\n",
    "allowable_features_to_split_on = data.data_vars.keys()\n",
    "\n",
    "feature, new_data, best_split = build_tree(y, [2, 1], total_data_array, allowable_features_to_split_on)\n",
    "levels = []\n",
    "levels.append([feature, new_data, best_split])\n",
    "print best_split\n",
    "print \"Length of new data \", len(new_data)\n",
    "possible_y_vals_array = []\n",
    "for split in best_split:\n",
    "    new_list = []\n",
    "    for y_val in split.keys():\n",
    "        new_list.append(y_val)\n",
    "    possible_y_vals_array.append(new_list)\n",
    "    \n",
    "# Edit the features allowed to split on\n",
    "\n",
    "'''\n",
    "for data in new_data[1]:\n",
    "    print data.data_vars[\"sex\"]\n",
    "'''\n",
    "\n",
    "allowable_features_to_split_on.remove(\"has_heart_disease\")\n",
    "    \n",
    "\n",
    "\n",
    "tree_array = []\n",
    "# Process each feature \n",
    "# level = [0: feature, 1: new_data, 2: best_split]\n",
    "new_levels = []\n",
    "tree_is_finished = False\n",
    "processed = 0\n",
    "while not tree_is_finished:\n",
    "    \n",
    "    for level in levels:\n",
    "        '''\n",
    "        feature = level[0]\n",
    "        for level in levels:\n",
    "            print level[0]\n",
    "        print \"Processing feature: \", level[0]\n",
    "        new_data = level[1]\n",
    "        best_split = level[2]\n",
    "        print \"Split is : \", best_split\n",
    "        print 'Length of New Data',len(new_data)\n",
    "        '''\n",
    "\n",
    "        # Process each split for the feature\n",
    "        for branch_index in range(len(level[1])):\n",
    "            print \"Processing the branch for:\"\n",
    "            print \"\\t Feature = \", level[0]\n",
    "            print \"\\t Split   = \", level[2][branch_index]\n",
    "            data_for_next_call = level[1][branch_index]\n",
    "            print \"\\t Length of New Data   = \", len(data_for_next_call)\n",
    "            '''\n",
    "            if len(level[2][branch_index]) > 1:\n",
    "                data_for_next_call = level[1][branch_index]\n",
    "                print\n",
    "            else: \n",
    "                print \"\\t Length of New Data   = \", len(level[1][branch_index][0])\n",
    "            '''\n",
    "            # Edit the allowable features to split on since if you split on a nominal feature and only have \n",
    "            # one value for that nominal feature in the branch, you shouldn't split on it again\n",
    "            if features_and_types[feature] == \"nominal\" and len(possible_y_vals_array[0]) == 1:\n",
    "                allowable_features_to_split_on.remove(level[0])\n",
    "            # Call build_tree for next partition\n",
    "            #print \"Allowable_features_to_split_on:\"\n",
    "            #print allowable_features_to_split_on\n",
    "            #print \"y is : \"+feature\n",
    "            if len(allowable_features_to_split_on) != 0:\n",
    "                #print \"Possible_y_vals:\"\n",
    "                #print possible_y_vals_array\n",
    "                #print \"branch index: \"+str(branch_index)\n",
    "\n",
    "\n",
    "                \n",
    "                print \"Length of the data for the next call: \", len(data_for_next_call)\n",
    "\n",
    "                possible_y_values = []\n",
    "                for pt in data_for_next_call:\n",
    "                        y_value = pt.data_vars[level[0]]\n",
    "                        if y_value not in possible_y_values:\n",
    "                            possible_y_values.append(y_value)\n",
    "\n",
    "                #print \"Allowable features to split on:\"\n",
    "                #print allowable_features_to_split_on\n",
    "                #print \"Level[0]\"\n",
    "                #print level[0]\n",
    "                #print \"Possible y values:\"\n",
    "                #print possible_y_values\n",
    "                #print \"new data of branch index:\"\n",
    "                #print new_data[branch_index]\n",
    "\n",
    "                '''\n",
    "                if level[0] == \"chest_pain_type\":\n",
    "                    for data in data_for_next_call:\n",
    "                        print_data(data)\n",
    "                '''\n",
    "\n",
    "                next_feature, next_new_data, next_best_split = build_tree(level[0], possible_y_values, data_for_next_call, allowable_features_to_split_on)\n",
    "\n",
    "                print \"*** Next Feature:\"\n",
    "                print next_feature\n",
    "                #print \"*** Next New Data:\"\n",
    "                #print next_new_data\n",
    "                print \"*** Next Best Split:\"\n",
    "                print next_best_split\n",
    "                print \"*** Length of Next New Data:\"\n",
    "                print len(next_new_data)\n",
    "                \n",
    "                new_levels.append([next_feature, next_new_data, next_best_split])\n",
    "\n",
    "\n",
    "                # Put feature back in allowable features because it's possible we might need it later\n",
    "                allowable_features_to_split_on.append(level[0])\n",
    "        \n",
    "    # Levels has finished processing\n",
    "    for level in levels:\n",
    "        # 1st Arg: Splitting_feature\n",
    "        # 2nd Arg: Feature_value\n",
    "        for split in level[2]:\n",
    "            tree_array.append(Node(level[0], split))\n",
    "    '''\n",
    "    for node in tree_array:\n",
    "        print \"new Node:\"\n",
    "        print node.splitting_feature\n",
    "        print node.feature_value\n",
    "    '''\n",
    "    '''\n",
    "    print \"NEW LEVELS:\"\n",
    "    for e in new_levels:\n",
    "        print \"Level: \",e[0]\n",
    "        print \"Length of Data: \",len(e[1])        \n",
    "        print \"Split: \",e[2]\n",
    "    '''\n",
    "    del levels[:]\n",
    "    \n",
    "    # Set the levels equal to everything in the new_levels\n",
    "    for s in new_levels:\n",
    "        levels.append(s)\n",
    "    \n",
    "    print \"BEFORE DELETING ANYTHING\"\n",
    "    for e in levels:\n",
    "        print e[0]\n",
    "        \n",
    "    # Also delete the ones you've processed in new_levels\n",
    "    for i in range(processed*3):\n",
    "        print \"i = \", i\n",
    "        print \"Removing \", levels[i][0], \" from the list.\"\n",
    "        del levels[i]\n",
    "        \n",
    "    \n",
    "    print \"ASSIGNING NEW LEVELS\"\n",
    "    for e in levels:\n",
    "        print e[0]\n",
    "    processed += 1\n",
    "    if processed == 3:\n",
    "        break\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
