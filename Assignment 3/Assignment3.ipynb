{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Assignment 3:  Decision Tree Implementation\n",
    "*Margaret Thomann - February 17, 2018 *\n",
    "\n",
    "In this assignment, I will construct a decision tree from the data provided about heart disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Reading the data and assigning counts to arrays and Data class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Data Class \n",
    "A Data class will be instantiated for each line of the data.  It will then be added to one of two arrays (explained later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class Data:\n",
    "    def __init__(self, class_value):\n",
    "        self.class_value = class_value\n",
    "        self.data_vars = OrderedDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Calculate Information Gain for Each Feature\n",
    "The below function can be used to determine the information gain for a given data and hypothesis (passed in as a string - x and y).  Information Gain can be represented as: Infgain(Y|X_K) = H(Y) - H(Y|X_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Data processed\n",
      "-----------------\n",
      "\t120 = # Of People with Heart Disease\n",
      "\t150 = # Of People without Heart Disease\n"
     ]
    }
   ],
   "source": [
    "# Arrays for the Data instances\n",
    "#     absence_heart_array  : contains all Data instantiations where heart disease is absent\n",
    "#     presence_heart_array : contains all Data instantiations where heart disease is absent\n",
    "absence_heart_array = []\n",
    "presence_heart_array = []\n",
    "total_data_array = []\n",
    "\n",
    "# Classify the features according to their type: nominal or continuous\n",
    "indices_for_nominal = [10, 1, 5, 8, 6, 2, 12]\n",
    "indices_for_continuous = [0,3,4,7,9,11]\n",
    "feature_names = [\"age\", \"sex\", \"chest_pain_type\", \"resting_blood_pressure\", \"serum_cholesterol\", \"fasting_blood_sugar\",\n",
    "                \"resting_electrocardiographic_results\", \"maximum_heart_rate_achieved\", \"exercise_induced_angina\",\n",
    "                \"oldpeak\", \"slope_peak_exercise\", \"number_of_major_vessels\", \"thal\", \"has_heart_disease\"]\n",
    "features_and_types = OrderedDict()\n",
    "for feature in feature_names:\n",
    "    if feature_names.index(feature) in indices_for_continuous:\n",
    "        features_and_types[feature] = \"continuous\"\n",
    "    else:\n",
    "        features_and_types[feature] = \"nominal\"\n",
    "\n",
    "# Process the data and store it in the arrays\n",
    "data = open('heart.data.txt')\n",
    "for line in data.readlines():\n",
    "    feature_value_list = line.split()\n",
    "    has_heart_disease = int(feature_value_list[-1])\n",
    "    data = Data(has_heart_disease)\n",
    "    counter = 0\n",
    "    feature_dict = OrderedDict()\n",
    "    for feature in feature_value_list:\n",
    "        data.data_vars[feature_names[counter]] = float(feature)\n",
    "        counter += 1\n",
    "    if has_heart_disease == 2:\n",
    "        presence_heart_array.append(data)\n",
    "    elif has_heart_disease == 1:\n",
    "        absence_heart_array.append(data)\n",
    "    total_data_array.append(data)\n",
    "\n",
    "presence_heart_array_num = len(presence_heart_array)\n",
    "absence_heart_array_num = len(absence_heart_array)\n",
    "print \"✔ Data processed\"\n",
    "print \"-----------------\"\n",
    "print \"\\t\" + str(presence_heart_array_num) + \" = # Of People with Heart Disease\"\n",
    "print \"\\t\" + str(absence_heart_array_num) + \" = # Of People without Heart Disease\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Utility Printer Function to Print out Data\n",
    "The below function can be called with a Data object as its input to print out that data objects contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_data(data_obj):\n",
    "    print \"_____________\"\n",
    "    for feature in data_obj.data_vars.keys():\n",
    "        print data_obj.data_vars[feature], \": \", feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_info_gain(y, y_values, x, buckets, considered_data):\n",
    "        \n",
    "    '''\n",
    "    if x == \"sex\":\n",
    "        print \"sex vals!!!\"\n",
    "        for data_point in considered_data:\n",
    "            print data_point.data_vars[\"sex\"]\n",
    "        #print \"y_values:\"\n",
    "        #print y_values\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    if y == \"chest_pain_type\":\n",
    "        print \"y_values:\"\n",
    "        print y_values\n",
    "    '''\n",
    "    \n",
    "    #print \"Calling compute_info_gain with y = \"+y+\" and x = \"+x\n",
    "    \n",
    "        \n",
    "    # Define dicts for the counts\n",
    "    positive_y_counts = {}\n",
    "    positive_x_counts = {}\n",
    "    negative_y_counts = {}\n",
    "    negative_x_counts = {}\n",
    "    \n",
    "    # Get the bucket values\n",
    "    for bucket in buckets:\n",
    "        # Convert to string\n",
    "        s = \"\"\n",
    "        for num in list(set(bucket)):\n",
    "            s += (str(num)+ \" \")\n",
    "        positive_x_counts[s] = 0\n",
    "        negative_x_counts[s] = 0\n",
    "    \n",
    "    \n",
    "    for val in y_values:\n",
    "        positive_y_counts[str(float(val))] = 0\n",
    "        negative_y_counts[str(float(val))] = 0\n",
    "    \n",
    "    y_denom = 0\n",
    "    for data in considered_data:\n",
    "        y_denom += 1\n",
    "        has_heart_disease = data.data_vars[\"has_heart_disease\"]\n",
    "        y_value = data.data_vars[y]\n",
    "        x_value = data.data_vars[x]\n",
    "        array_x = []\n",
    "        array_y = []\n",
    "        if has_heart_disease == 1:\n",
    "            # Update all of the negative arrays\n",
    "            array_x = negative_x_counts\n",
    "            array_y = negative_y_counts\n",
    "        elif has_heart_disease == 2:\n",
    "            # Update all of the positive arrays\n",
    "            array_x = positive_x_counts\n",
    "            array_y = positive_y_counts\n",
    "            \n",
    "        # Value is not in dictionary yet\n",
    "        # so set the occurrence for that value to 1\n",
    "        '''\n",
    "        if value not in array_y.keys():\n",
    "            array_y[y_value] = 1 \n",
    "        # Value is already in dictionary\n",
    "        # so increase the occurrence count for that value by 1\n",
    "        else:\n",
    "            current_count_for_value = array_y[y_value]\n",
    "            array_y.update({y_value:current_count_for_value+1})\n",
    "        '''\n",
    "        for key in array_x.keys():\n",
    "            if str(x_value)+\" \" in key:\n",
    "                current_count_for_value = array_x[key]\n",
    "                array_x.update({key:current_count_for_value+1})\n",
    "                \n",
    "        for key in array_y.keys():\n",
    "            if str(y_value) in key:\n",
    "                current_count_for_value = array_y[key]\n",
    "                array_y.update({key:current_count_for_value+1})\n",
    "\n",
    "    '''\n",
    "    y_denom = 0\n",
    "    \n",
    "    for data in presence_heart_array:\n",
    "        y_denom += 1\n",
    "        value = data.data_vars[y]\n",
    "        \n",
    "        # Value is not in dictionary yet\n",
    "        # so set the occurrence for that value to 1\n",
    "        if value not in positive_y_counts.keys():\n",
    "            positive_y_counts[value] = 1 \n",
    "        # Value is already in dictionary\n",
    "        # so increase the occurrence count for that value by 1\n",
    "        else:\n",
    "            current_count_for_value = positive_y_counts[value]\n",
    "            positive_y_counts.update({value:current_count_for_value+1})\n",
    "            \n",
    "        # Same thing is done for processing x:\n",
    "        x_value = data.data_vars[x]\n",
    "        for key in positive_x_counts.keys():\n",
    "            if str(x_value)+\" \" in key:\n",
    "                current_count_for_value = positive_x_counts[key]\n",
    "                positive_x_counts.update({key:current_count_for_value+1})\n",
    "            \n",
    "    \n",
    "    for data in absence_heart_array:\n",
    "        y_denom += 1\n",
    "        value = data.data_vars[y]\n",
    "        \n",
    "        # Value is not in dictionary yet\n",
    "        # so set the occurrence for that value to 1\n",
    "        if value not in negative_y_counts.keys():\n",
    "            negative_y_counts[value] = 1 \n",
    "        # Value is already in dictionary\n",
    "        # so increase the occurrence count for that value by 1\n",
    "        else:\n",
    "            current_count_for_value = negative_y_counts[value]\n",
    "            negative_y_counts.update({value:current_count_for_value+1})\n",
    "            \n",
    "        # Same thing is done for processing x:\n",
    "        x_value = data.data_vars[x]\n",
    "        for key in negative_x_counts.keys():\n",
    "            if str(x_value)+\" \" in key:\n",
    "                current_count_for_value = negative_x_counts[key]\n",
    "                negative_x_counts.update({key:current_count_for_value+1})\n",
    "                \n",
    "    '''\n",
    "    '''\n",
    "    if x == \"sex\":\n",
    "        print \"postive_y_counts array\"\n",
    "        print positive_y_counts\n",
    "        print \"negative_y_counts array\"\n",
    "        print negative_y_counts\n",
    "    '''   \n",
    "            \n",
    "    h_of_y = 0\n",
    "    for count in positive_y_counts.values():\n",
    "        '''\n",
    "        if y == \"chest_pain_type\":\n",
    "            print \"positive_y_count: \"+str(count)\n",
    "        '''\n",
    "        \n",
    "        entropy = 0\n",
    "        if count != 0:\n",
    "            p = float(float(count)/float(y_denom))\n",
    "            entropy = -1 * p * (math.log(p, 2))\n",
    "        h_of_y += entropy\n",
    "    for count in negative_y_counts.values():\n",
    "\n",
    "        '''\n",
    "        if y == \"chest_pain_type\":\n",
    "            print \"negative_y_count: \"+str(count)\n",
    "        '''\n",
    "    \n",
    "        entropy = 0\n",
    "        if count != 0:\n",
    "            p = float(float(count)/float(y_denom))\n",
    "            entropy = -1 * p * (math.log(p, 2))\n",
    "        h_of_y += entropy\n",
    "    \n",
    "    h_of_y_given_x = 0\n",
    "    for feature_value in positive_x_counts.keys():\n",
    "        '''\n",
    "        if x == \"sex\":\n",
    "            print \"positive_x_counts: \"+str(positive_x_counts[feature_value])+\" for feature value: \"+str(feature_value)\n",
    "            print \"negative_x_counts: \"+str(negative_x_counts[feature_value])+\" for feature value: \"+str(feature_value)        \n",
    "        '''\n",
    "        entropy_positive = 0\n",
    "        entropy_negative = 0\n",
    "        sum_of_values = positive_x_counts[feature_value] + negative_x_counts[feature_value] \n",
    "        fraction = float(float(sum_of_values)/float(y_denom))\n",
    "        if positive_x_counts[feature_value] != 0:\n",
    "            p_positive = float(float(positive_x_counts[feature_value])/float(sum_of_values)) \n",
    "            entropy_positive = p_positive * (math.log(p_positive, 2))\n",
    "        if negative_x_counts[feature_value] != 0:\n",
    "            p_negative = float(float(negative_x_counts[feature_value])/float(sum_of_values)) \n",
    "            entropy_negative = p_negative * (math.log(p_negative, 2))\n",
    "        \n",
    "        h_of_y_given_x += fraction*(entropy_positive+entropy_negative)\n",
    "    \n",
    "    '''\n",
    "    if x == \"sex\":\n",
    "        print \"h_of_y:\"+str(h_of_y)\n",
    "        print \"h_of_y_given_x:\"+str(h_of_y_given_x)\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    if h_of_y_given_x < 0:\n",
    "        info_gain = h_of_y - (h_of_y_given_x)\n",
    "        return info_gain\n",
    "    else:\n",
    "        info_gain = h_of_y - (-1*h_of_y_given_x)\n",
    "        return info_gain\n",
    "    '''\n",
    "    #print \"h of y: \", h_of_y\n",
    "    #print \"h of y given x: \", h_of_y_given_x\n",
    "    info_gain = h_of_y + h_of_y_given_x\n",
    "    return info_gain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Utility Function used to generate all of the possible splits\n",
    "Credit:  https://stackoverflow.com/questions/19368375/set-partitions-in-python/30134039#30134039"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def partition(collection):\n",
    "    if len(collection) == 1:\n",
    "        yield [ collection ]\n",
    "        return\n",
    "\n",
    "    first = collection[0]\n",
    "    for smaller in partition(collection[1:]):\n",
    "        # insert `first` in each of the subpartition's subsets\n",
    "        for n, subset in enumerate(smaller):\n",
    "            yield smaller[:n] + [[ first ] + subset]  + smaller[n+1:]\n",
    "        # put `first` in its own subset \n",
    "        yield [ [ first ] ] + smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Determine possible splits\n",
    "These will be used for the information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get_continuous_binary_split(feature):     \n",
    "#    Gets a feature and splits the data for that feature in all the possible ways to split that\n",
    "#    data into two buckets.  It returns a dictionary where the key is the number it split on and the\n",
    "#    value is a list of two lists.  The first element of the list is all of the elements less than\n",
    "#    the split and the second element of the list is a list of all of the elements greater than or\n",
    "#    equal to the split.\n",
    "#    For example:\n",
    "#           feature = \"age\"\n",
    "#           splits_dict = {50: [[20,40,43...],[50,60,61...]], 60: [[57,45,59...],[60,61,63...]]}\n",
    "def get_continuous_binary_split(feature, considered_data):\n",
    "    # Ensure the function is being called only with continuous features\n",
    "    if features_and_types[feature] != \"continuous\":\n",
    "        raise ValueError('Error in get_continuous_binary_split: input feature is not continuous.')\n",
    "        return\n",
    "    \n",
    "    # Create a list of the possible splits\n",
    "    splits = []\n",
    "    counts_for_feature_values = {}\n",
    "    \n",
    "    #print \"The Length of the Considered Data: \",len(considered_data)\n",
    "    \n",
    "    for data in considered_data:\n",
    "        feature_value = float(data.data_vars[feature])\n",
    "        if feature_value not in splits:\n",
    "            splits.append(feature_value)\n",
    "        if feature_value not in counts_for_feature_values:\n",
    "            counts_for_feature_values[feature_value] = 1\n",
    "        else:\n",
    "            current = counts_for_feature_values[feature_value]\n",
    "            counts_for_feature_values[feature_value] = current + 1\n",
    "    \n",
    "    # Process the splits and create the less than list and greater than or equal to list for each\n",
    "    splits_list = []\n",
    "    for split in splits:\n",
    "        lt_split_feature_values = []\n",
    "        gtequal_split_feature_values = []\n",
    "        for data in considered_data:\n",
    "            feature_value = float(data.data_vars[feature])\n",
    "            if feature_value < split:\n",
    "                lt_split_feature_values.append(feature_value)\n",
    "            else:\n",
    "                gtequal_split_feature_values.append(feature_value)\n",
    "        #if len(lt_split_feature_values) != 0:\n",
    "        splits_list.append([lt_split_feature_values, gtequal_split_feature_values])\n",
    "            \n",
    "    # Generate a list of partition dicts\n",
    "    # Each list element will be a list of dictionaries\n",
    "    # The dictionaries will contain the feature_value and the number of times it occurs\n",
    "    # The list of these dictionaries will be split up by partition\n",
    "    partitions_and_values = []\n",
    "    for partition in splits_list:\n",
    "        split_inclusive = []\n",
    "        for l in partition:\n",
    "            set_l = set(l)\n",
    "            count_dict = {}\n",
    "            for feature_value in set_l:\n",
    "                num_of_feature_value_occurrences = counts_for_feature_values[feature_value]\n",
    "                count_dict[feature_value] = num_of_feature_value_occurrences\n",
    "            split_inclusive.append(count_dict)\n",
    "        partitions_and_values.append(split_inclusive)\n",
    "            \n",
    "    return partitions_and_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "# get_nominal_split(feature):     \n",
    "#    Gets a nominal feature and splits the data for that feature in all the possible ways to split that\n",
    "#    data into every possible number of buckets.  It returns a dictionary where the key is the number it \n",
    "#    split on and the value is a list of the lists it generated from the split.  \n",
    "\n",
    "def split_list(data, n):\n",
    "    from itertools import combinations, chain\n",
    "    for splits in combinations(range(1, len(data)), n-1):\n",
    "        result = []\n",
    "        prev = None\n",
    "        for split in chain(splits, [None]):\n",
    "            result.append(data[prev:split])\n",
    "            prev = split\n",
    "        yield result\n",
    "        \n",
    "def get_nominal_split(feature, considered_data):\n",
    "    # Ensure the function is being called only with nominal features\n",
    "    if features_and_types[feature] != \"nominal\":\n",
    "        raise ValueError('Error in get_nominal_split: input feature is not nominal.')\n",
    "        return\n",
    "    \n",
    "    #print \"get_nominal_split called with feature: \"+feature\n",
    "    # Determine the unique values for the feature and the counts for the feature_value\n",
    "    splits = {}\n",
    "    split_nums = []\n",
    "    for data in considered_data:\n",
    "        feature_value = float(data.data_vars[feature])\n",
    "        if feature_value not in splits:\n",
    "            split_nums.append(feature_value)\n",
    "            splits[feature_value] = 1\n",
    "        else:\n",
    "            current_num = splits[feature_value]\n",
    "            splits[feature_value] = current_num+1\n",
    "    \n",
    "    #print \"possible values for feature:\"\n",
    "    #print split_nums\n",
    "\n",
    "    # Generate all the possible ways to partition the numbers  \n",
    "    # possible_partitions is a list of tuples where the first element of the tuple\n",
    "    # is the kth possible partition and the second element of the tuple is a list\n",
    "    # of lists of partitions\n",
    "    possible_partitions = []\n",
    "    for p in partition(split_nums):\n",
    "        # Be sure to remove the split where all values are placed in one bucket\n",
    "        if (len(sorted(p)) != 1) or ((len(sorted(p)) == 1) and (len(split_nums) == 1)):\n",
    "            possible_partitions.append(sorted(p))\n",
    "    #print \"possible_partitions:\"\n",
    "    #print possible_partitions\n",
    "                \n",
    "    # Generate a list of partition dicts\n",
    "    # Each list element will be a list of dictionaries\n",
    "    # The dictionaries will contain the feature_value and the number of times it occurs\n",
    "    # The list of these dictionaries will be split up by partition\n",
    "    partitions_and_values = []\n",
    "    for split_lists in possible_partitions:\n",
    "        split_dicts = []\n",
    "        for bucket in split_lists:\n",
    "            bucket_dict = {}\n",
    "            for feature_value in bucket:\n",
    "                num_of_feature_value_occurrences = splits[feature_value]\n",
    "                bucket_dict[feature_value] = num_of_feature_value_occurrences\n",
    "            split_dicts.append(bucket_dict)\n",
    "        partitions_and_values.append(split_dicts)\n",
    "    return partitions_and_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Compute the best split\n",
    "Checks the information gain for all the possible splits for a certain feature and reports the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_best_split(y, y_values, feature, considered_data):\n",
    "    # Check if the feature is nominal or continuous and split accordingly\n",
    "    splits = []\n",
    "    if features_and_types[feature] == \"nominal\":\n",
    "        \n",
    "        splits = get_nominal_split(feature, considered_data)\n",
    "    else:\n",
    "        splits = get_continuous_binary_split(feature, considered_data)\n",
    "    \n",
    "    # Create a dictionary where the key is the information gain from a particular\n",
    "    # split and the value is that particular split\n",
    "    info_gains = {}\n",
    "    for split in splits:\n",
    "        info_gain = compute_info_gain(y, y_values, feature, split, considered_data)\n",
    "        info_gains[info_gain] = split\n",
    "        \n",
    "    # Process the dictionary and determine the highest info gain\n",
    "    #print \"info-gains\"\n",
    "    #print \"compute_best_info_gain called with: \"+feature\n",
    "    #print \"considered_data\"\n",
    "    #print considered_data\n",
    "    #print info_gains\n",
    "    max_info_gain = max(info_gains, key=float)\n",
    "    split_yielding_max_info_gain = info_gains[max_info_gain]\n",
    "    \n",
    "    # Print the maximum info gain and corresponding split\n",
    "    #print \"FEATURE: \"+feature\n",
    "    #print \"Greatest Info Gain is: \"+str(max_info_gain)\n",
    "    #print \"from split:\"\n",
    "    #for split in split_yielding_max_info_gain:\n",
    "        #print sorted(set(split))\n",
    "    return max_info_gain, split_yielding_max_info_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tree Class\n",
    "Set up classes for the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, splitting_feature, feature_value, feature_value_occurrence, data):\n",
    "        self.splitting_feature = splitting_feature\n",
    "        self.feature_value = feature_value\n",
    "        self.feature_value_occurrence = feature_value_occurrence\n",
    "        self.data = data\n",
    "        self.id = \"\"\n",
    "        self.children = []\n",
    "\n",
    "    def add_child(self, obj):\n",
    "        self.children.append(obj)\n",
    "        \n",
    "    def print_node(self, node):\n",
    "        print \"\\t splitting_feature: \", node.splitting_feature\n",
    "        print \"\\t feature_value: \", node.feature_value\n",
    "        print \"\\t feature_value_occurrence: \", node.feature_value_occurrence\n",
    "    \n",
    "    def print_children(self):\n",
    "        counter = 1\n",
    "        for child in self.children:\n",
    "            print counter, \":\"\n",
    "            self.print_node(child)\n",
    "            counter += 1\n",
    "    \n",
    "    def print_node_and_children(self):\n",
    "        print \"*   NODE  *\"\n",
    "        self.print_node(self)\n",
    "        print \"* CHILDREN *\"\n",
    "        self.print_children()\n",
    "        \n",
    "    def is_equal(self, leaf):\n",
    "        print \"comparing: \", self.splitting_feature, \" with: \", leaf.splitting_feature, \" and result is: \", (self.splitting_feature == leaf.splitting_feature)\n",
    "        print \"comparing: \", self.feature_value, \" with: \", leaf.feature_value,\" and result is: \", (self.feature_value == leaf.feature_value)\n",
    "        print \"comparing: \", self.feature_value_occurrence, \" with: \", leaf.feature_value_occurrence, \" and result is: \", (self.feature_value_occurrence == leaf.feature_value_occurrence)\n",
    "        print \"comparing: children and result is: \",  (len(self.children) == len(leaf.children))\n",
    "        print \"comparing data and data: \", (len(self.data) == len(leaf.data))\n",
    "        print \"length of children self: \", len(self.children)\n",
    "        print \"length of children data: \", len(leaf.children)\n",
    "        if (self.splitting_feature == leaf.splitting_feature) and (self.feature_value == leaf.feature_value) and (self.feature_value_occurrence == leaf.feature_value_occurrence):\n",
    "            print \"returning true for \", self.splitting_feature, \" and \", leaf.splitting_feature\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Split Data\n",
    "Split the data and return a list of arrays of the split data given the feature and the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def split_data(feature, splits, considered_data):\n",
    "    #print \"Splitting data for feature: \"+feature\n",
    "    #print \"split is: \"\n",
    "    #print splits\n",
    "    data = []\n",
    "    for split in splits:\n",
    "        data.append([])\n",
    "    separate_buckets = {}\n",
    "    for data in considered_data:\n",
    "        feature_value = float(data.data_vars[feature])\n",
    "        if feature_value not in separate_buckets.keys():\n",
    "            new_list = []\n",
    "            new_list.append(data)\n",
    "            separate_buckets[feature_value] = new_list\n",
    "        else:\n",
    "            current_list = separate_buckets[feature_value]\n",
    "            current_list.append(data)\n",
    "            separate_buckets[feature_value] = current_list\n",
    "        \n",
    "    #splits = [{3.0: 152}, {6.0: 14}, {7.0: 16}]\n",
    "    #OR splits = [{3.0: 152}, [{6.0: 14}, {7.0: 16}]]\n",
    "            \n",
    "    new_data = []\n",
    "    for split in splits:\n",
    "        \n",
    "        #print \"Processing split: \", split\n",
    "        \n",
    "        # {0.0: 7}\n",
    "        # {1.0: 4, 2.0: 2, 3.0: 1}\n",
    "        \n",
    "        # Convert it to a list\n",
    "        split_list = []\n",
    "        if len(split) == 1:\n",
    "            \n",
    "            split_list.append(split)\n",
    "        else:\n",
    "            split_list.append(split)\n",
    "            '''\n",
    "            for each_value in split:\n",
    "                print \"Appending each value: \", each_value\n",
    "                split_list.append(each_value)\n",
    "            '''\n",
    "        \n",
    "        # Prepare a list for the data of that split\n",
    "        data_for_split = []\n",
    "        \n",
    "        # Get all the valid feature_values for that particular split\n",
    "        valid_feature_values = []\n",
    "        #print \"SPLIT LIST IN SPLIT DATA: \", split_list\n",
    "        for split_dict in split_list:\n",
    "            for feature_value_key in split_dict.keys():\n",
    "                valid_feature_values.append(feature_value_key)\n",
    "        \n",
    "        # Process the valid feature values, find matching data, and add it to the data for that split\n",
    "        for feature_value in valid_feature_values:\n",
    "            for data_pt in separate_buckets[feature_value]:\n",
    "                data_for_split.append(data_pt)\n",
    "        \n",
    "        # Add the data relevant for a certain split group to the new data array\n",
    "        new_data.append(data_for_split)\n",
    "        '''\n",
    "        if feature == \"number_of_major_vessels\":\n",
    "            print \"Length of data for split: \", len(data_for_split)\n",
    "            '''\n",
    "    '''\n",
    "    if feature == \"number_of_major_vessels\":\n",
    "        print \"Length of data returned for 1, 2, 3 : \", len(new_data[1])\n",
    "    '''\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Recurse to build the tree\n",
    "Used the information gain function to determine the best splits for each node of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_tree(y, y_values, data, allowable_features_to_split_on):\n",
    "    info_gains = {}\n",
    "    split_yielding_max_info_gain = []\n",
    "    for feature in allowable_features_to_split_on:\n",
    "        if feature != y:\n",
    "            info_gain, split_yielding_max_info_gain = compute_best_split(y, y_values, feature, data)\n",
    "            info_gains[feature] = [info_gain, split_yielding_max_info_gain]\n",
    "\n",
    "    #print info_gains\n",
    "\n",
    "    # Printing out the information gains for each feature\n",
    "    '''\n",
    "    for feature in info_gains.keys():\n",
    "        print str(info_gains[feature][0])+\" : \"+feature\n",
    "    '''\n",
    "\n",
    "    # Find the maximum in the info_gains returned\n",
    "    max_info_gain = 0\n",
    "    best_split = []\n",
    "    best_feature = \"\"\n",
    "    \n",
    "    # Check if they are all equal to 0\n",
    "    counter = 0\n",
    "    total = 0\n",
    "    for feature in info_gains.keys():\n",
    "        if info_gains[feature][0] == 0:\n",
    "            counter += 1\n",
    "        total += 1\n",
    "    if counter == total:\n",
    "        return\n",
    "    \n",
    "    #print info_gains.keys()\n",
    "    for feature in info_gains.keys():\n",
    "        if info_gains[feature][0] > max_info_gain:\n",
    "            max_info_gain = info_gains[feature][0]\n",
    "            best_split = info_gains[feature][1]\n",
    "            best_feature = feature\n",
    "\n",
    "    #print max_info_gain\n",
    "    #print best_feature\n",
    "    # Split on that feature and optimal split\n",
    "    new_data = split_data(best_feature, best_split, data)\n",
    "\n",
    "    # Define the new y based on this data\n",
    "    y = feature\n",
    "    \n",
    "    #print best_split\n",
    "    return best_feature, new_data, best_split\n",
    "\n",
    "\n",
    "\n",
    "    # Now we have to feed that new data into the tree and branch off into the leaves\n",
    "    # so we have three groups for thal which means it should branch into three leaves:\n",
    "    # one with 3, one with 6, and one with 7.\n",
    "    # And recursively continue branching\n",
    "    \n",
    "    \n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing:  thal  with: "
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'splitting_feature'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-486-d0a6f41ef9c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;31m# Assign the id but first check to see if you've already assigned it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0massigned_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massigned_leaf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malready_assigned_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mcurrent_leaf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massigned_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                     \u001b[0mcurrent_leaf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massigned_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-483-a9473a1c0c1f>\u001b[0m in \u001b[0;36mis_equal\u001b[0;34m(self, leaf)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mis_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"comparing: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitting_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" with: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitting_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" and result is: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitting_feature\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mleaf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitting_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"comparing: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" with: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" and result is: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_value\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mleaf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"comparing: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_value_occurrence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" with: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_value_occurrence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" and result is: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_value_occurrence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mleaf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_value_occurrence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'splitting_feature'"
     ]
    }
   ],
   "source": [
    "y = \"has_heart_disease\"\n",
    "allowable_features_to_split_on = data.data_vars.keys()\n",
    "\n",
    "feature, new_data, best_split = build_tree(y, [2, 1], total_data_array, allowable_features_to_split_on)\n",
    "levels = []\n",
    "levels.append([feature, new_data, best_split])\n",
    "#print best_split\n",
    "#print \"Length of new data \", len(new_data)\n",
    "possible_y_vals_array = []\n",
    "for split in best_split:\n",
    "    new_list = []\n",
    "    for y_val in split.keys():\n",
    "        new_list.append(y_val)\n",
    "    possible_y_vals_array.append(new_list)\n",
    "    \n",
    "# Edit the features allowed to split on\n",
    "\n",
    "'''\n",
    "for data in new_data[1]:\n",
    "    print data.data_vars[\"sex\"]\n",
    "'''\n",
    "\n",
    "allowable_features_to_split_on.remove(\"has_heart_disease\")\n",
    "    \n",
    "\n",
    "\n",
    "tree_array = []\n",
    "# Process each feature \n",
    "# level = [0: feature, 1: new_data, 2: best_split]\n",
    "new_levels = []\n",
    "tree_is_finished = False\n",
    "processed = 0\n",
    "already_assigned_ids = {}\n",
    "# splitting_feature, feature_value, feature_value_occurrence, data):\n",
    "node = Node(feature, \"has_heart_disease\", 270, new_data)\n",
    "\n",
    "already_assigned_ids[str()] = \"dummy\"\n",
    "while not tree_is_finished:\n",
    "    \n",
    "    for level in levels:\n",
    "        '''\n",
    "        feature = level[0]\n",
    "        for level in levels:\n",
    "            print level[0]\n",
    "        print \"Processing feature: \", level[0]\n",
    "        new_data = level[1]\n",
    "        best_split = level[2]\n",
    "        print \"Split is : \", best_split\n",
    "        print 'Length of New Data',len(new_data)\n",
    "        '''\n",
    "        \n",
    "        \n",
    "\n",
    "        # Process each split for the feature\n",
    "        for branch_index in range(len(level[1])):\n",
    "            \n",
    "            #for split in level[2]:\n",
    "            #print \"New Node:\"\n",
    "            #print \"\\t splitting_feature: \", level[0]\n",
    "            #print \"\\t feature_value: \", level[2][branch_index].keys()\n",
    "            #print \"\\t feature_value_occurrence: \", sum(level[2][branch_index].values())\n",
    "            current_leaf = Node(level[0], level[2][branch_index].keys(), sum(level[2][branch_index].values()),level[1][branch_index])\n",
    "            \n",
    "            # NEW ADDITION\n",
    "            # Assign the id but first check to see if you've already assigned it\n",
    "            for assigned_id, assigned_leaf in already_assigned_ids.iteritems():\n",
    "                if current_leaf.is_equal(assigned_leaf):\n",
    "                    current_leaf.id = assigned_id\n",
    "                else:\n",
    "                    unique_id = str(uuid.uuid4())\n",
    "                    current_leaf.id = unique_id\n",
    "                    already_assigned_ids[unique_id] = current_leaf\n",
    "            # END NEW ADDITION\n",
    "            \n",
    "            tree_array.append(current_leaf)\n",
    "            \n",
    "            print \"Processing the branch for:\"\n",
    "            print \"\\t Feature = \", level[0]\n",
    "            print \"\\t Split   = \", level[2][branch_index]\n",
    "            data_for_next_call = level[1][branch_index]\n",
    "            print \"\\t Length of New Data   = \", len(data_for_next_call)\n",
    "            '''\n",
    "            if len(level[2][branch_index]) > 1:\n",
    "                data_for_next_call = level[1][branch_index]\n",
    "                print\n",
    "            else: \n",
    "                print \"\\t Length of New Data   = \", len(level[1][branch_index][0])\n",
    "            '''\n",
    "            # Edit the allowable features to split on since if you split on a nominal feature and only have \n",
    "            # one value for that nominal feature in the branch, you shouldn't split on it again\n",
    "            if features_and_types[feature] == \"nominal\" and len(possible_y_vals_array[0]) == 1 and level[0] in allowable_features_to_split_on:\n",
    "                print \"Allowable features to split on: \", allowable_features_to_split_on\n",
    "                print \"Trying to remove: \", level[0]\n",
    "                allowable_features_to_split_on.remove(level[0])\n",
    "            # Call build_tree for next partition\n",
    "            #print \"Allowable_features_to_split_on:\"\n",
    "            #print allowable_features_to_split_on\n",
    "            #print \"y is : \"+feature\n",
    "            if len(allowable_features_to_split_on) != 0:\n",
    "                #print \"Possible_y_vals:\"\n",
    "                #print possible_y_vals_array\n",
    "                #print \"branch index: \"+str(branch_index)\n",
    "\n",
    "\n",
    "                \n",
    "                print \"Length of the data for the next call: \", len(data_for_next_call)\n",
    "                if len(data_for_next_call) > 5:\n",
    "\n",
    "                    possible_y_values = []\n",
    "                    for pt in data_for_next_call:\n",
    "                            y_value = pt.data_vars[level[0]]\n",
    "                            if y_value not in possible_y_values:\n",
    "                                possible_y_values.append(y_value)\n",
    "\n",
    "                    #print \"Allowable features to split on:\"\n",
    "                    #print allowable_features_to_split_on\n",
    "                    #print \"Level[0]\"\n",
    "                    #print level[0]\n",
    "                    #print \"Possible y values:\"\n",
    "                    #print possible_y_values\n",
    "                    #print \"new data of branch index:\"\n",
    "                    #print new_data[branch_index]\n",
    "\n",
    "                    '''\n",
    "                    if level[0] == \"chest_pain_type\":\n",
    "                        for data in data_for_next_call:\n",
    "                            print_data(data)\n",
    "                    '''\n",
    "\n",
    "                    next_feature, next_new_data, next_best_split = build_tree(level[0], possible_y_values, data_for_next_call, allowable_features_to_split_on)\n",
    "\n",
    "                    print \"*** Next Feature:\"\n",
    "                    print next_feature\n",
    "                    #print \"*** Next New Data:\"\n",
    "                    #print next_new_data\n",
    "                    print \"*** Next Best Split:\"\n",
    "                    print next_best_split\n",
    "                    print \"*** Length of Next New Data:\"\n",
    "                    print len(next_new_data)\n",
    "\n",
    "                    # Add the children for the current leaf\n",
    "                    for child_index in range(len(next_new_data)):\n",
    "                        child_splitting_feature = next_feature\n",
    "                        child_feature_value = next_best_split[child_index].keys()\n",
    "                        child_feature_value_occurrence = sum(next_best_split[child_index].values())\n",
    "                        child = Node(child_splitting_feature, child_feature_value, child_feature_value_occurrence,next_new_data)\n",
    "                        \n",
    "                        # NEW ADDITION\n",
    "                        # Assign the id but first check to see if you've already assigned it\n",
    "                        for assigned_id, assigned_leaf in already_assigned_ids.iteritems():\n",
    "                            if child.is_equal(assigned_leaf):\n",
    "                                child.id = assigned_id\n",
    "                            else:\n",
    "                                unique_id = str(uuid.uuid4())\n",
    "                                child.id = unique_id\n",
    "                                already_assigned_ids[unique_id] = child\n",
    "                        # END NEW ADDITION\n",
    "                        \n",
    "                        current_leaf.add_child(child)\n",
    "\n",
    "                    new_levels.append([next_feature, next_new_data, next_best_split])\n",
    "\n",
    "\n",
    "                    # Put feature back in allowable features because it's possible we might need it later\n",
    "                    allowable_features_to_split_on.append(level[0])\n",
    "                else:\n",
    "                    print \"Skipping call for: \", level[0]\n",
    "                    print \"where split is \", level[2][branch_index]\n",
    "                    print \" and data length is \", len(data_for_next_call)\n",
    "            current_leaf.print_node_and_children()\n",
    "        \n",
    "    # Levels has finished processing\n",
    "    '''\n",
    "    for level in levels:\n",
    "        # Each node is structured:\n",
    "            # 1. splitting_feature\n",
    "            # 2. feature_value\n",
    "            # 3. feature_value_occurrence\n",
    "        for split in level[2]:\n",
    "            print \"New Node:\"\n",
    "            print \"\\t splitting_feature: \", level[0]\n",
    "            print \"\\t feature_value: \", split.keys()\n",
    "            print \"\\t feature_value_occurrence: \", sum(split.values())\n",
    "            tree_array.append(Node(level[0], split.keys(), split.values()))\n",
    "    '''\n",
    "    '''\n",
    "    for node in tree_array:\n",
    "        print \"new Node:\"\n",
    "        print node.splitting_feature\n",
    "        print node.feature_value\n",
    "    '''\n",
    "    '''\n",
    "    print \"NEW LEVELS:\"\n",
    "    for e in new_levels:\n",
    "        print \"Level: \",e[0]\n",
    "        print \"Length of Data: \",len(e[1])        \n",
    "        print \"Split: \",e[2]\n",
    "    '''\n",
    "    del levels[:]\n",
    "    \n",
    "    # Set the levels equal to everything in the new_levels\n",
    "    for s in new_levels:\n",
    "        levels.append(s)\n",
    "        \n",
    "    # Also delete the ones you've processed in new_levels\n",
    "    \n",
    "    del levels[0:processed*3]\n",
    "        \n",
    "    \n",
    "    print \"ASSIGNING NEW LEVELS\"\n",
    "    for e in levels:\n",
    "        print \"Feature: \", e[0]\n",
    "        for data_list in e[1]:\n",
    "            if len(data_list) <= 5:\n",
    "                print \"FLAG \"+e[0]+\" as finished!!!\"\n",
    "        \n",
    "    \n",
    "    processed += 1\n",
    "    if processed == 3:\n",
    "        print \n",
    "        break\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tree Visualization\n",
    "Use the tree_array and the graphviz algorithm to build a tree visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Imports for the visualization\n",
    "import uuid\n",
    "import sys\n",
    "!{sys.executable} -m pip install graphviz\n",
    "from graphviz import Digraph\n",
    "\n",
    "# Initialize the graph\n",
    "dot = Digraph(comment='Heart Data Decision Tree')\n",
    "\n",
    "'''\n",
    "# Assign unique ids for all of the tree nodes\n",
    "id_dict = OrderedDict()\n",
    "for leaf in tree_array:\n",
    "    unique_id = str(uuid.uuid4())\n",
    "    leaf.id = unique_id\n",
    "    id_dict[unique_id] = leaf\n",
    "    print unique_id, \" = \", leaf.splitting_feature\n",
    "    \n",
    "# Assign unique ids for all of the children tree nodes\n",
    "for leaf in tree_array:\n",
    "    for child in leaf.children:\n",
    "        # Check if the id has already been assigned for this child in the id_dict\n",
    "        for assigned_leaf_id, assigned_leaf in id_dict.iteritems():\n",
    "            if child.is_equal(assigned_leaf):# The id has already been assigned\n",
    "                child.id = assigned_leaf_id\n",
    "'''\n",
    "\n",
    "# Go through the tree nodes with unique ids and add them as nodes to the graph\n",
    "for leaf in already_assigned_ids.keys():\n",
    "    dot.node(str(leaf), already_assigned_ids[leaf].splitting_feature + \" = \" + ', '.join(str(e) for e in already_assigned_ids[leaf].feature_value))\n",
    "    \n",
    "# Go through the children for each and assign edges\n",
    "print \"Length of already assigned ids: \", len(already_assigned_ids.keys())\n",
    "for leaf in already_assigned_ids.keys():\n",
    "    print \"Processing leaf: \", already_assigned_ids[leaf].splitting_feature\n",
    "    for child in already_assigned_ids[leaf].children:\n",
    "        child_id = child.id\n",
    "        dot.edge(leaf, child_id, constraint='false')\n",
    "        print \"Processign child: \", child.splitting_feature\n",
    "        '''\n",
    "        for child_leaf in id_dict.keys():\n",
    "            is_equal_val = id_dict[child_leaf].is_equal(child)\n",
    "            if is_equal_val:\n",
    "                print \"Adding an edge between :\", id_dict[leaf].splitting_feature, \" and \", id_dict[child_leaf].splitting_feature\n",
    "                dot.edge(leaf, child_leaf, constraint='false')\n",
    "        '''\n",
    "            \n",
    "            \n",
    "        #print \"child_id\", child_id\n",
    "        #print [leaf, child_id]\n",
    "        #dot.edge(leaf, child_id, constraint='false')\n",
    "        '''\n",
    "        for key, value in id_dict.iteritems():\n",
    "            print \"Child = \", child.splitting_feature\n",
    "            print \"Value = \", value\n",
    "            if value == child:\n",
    "                print \"Connecting: \", id_leaf[leaf].splitting_feature, \" with \", key\n",
    "                dot.edges([leaf, key])\n",
    "        '''\n",
    "\n",
    "dot.render('heart-decision-tree.gv', view=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
