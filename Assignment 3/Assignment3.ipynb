{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Assignment 3:  Decision Tree Implementation\n",
    "*Margaret Thomann - February 17, 2018 *\n",
    "\n",
    "In this assignment, I will construct a decision tree from the data provided about heart disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Reading the data and assigning counts to arrays and Data class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Data Class \n",
    "A Data class will be instantiated for each line of the data.  It will then be added to one of two arrays (explained later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class Data:\n",
    "    def __init__(self, class_value):\n",
    "        self.class_value = class_value\n",
    "        self.data_vars = OrderedDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Calculate Information Gain for Each Feature\n",
    "The below function can be used to determine the information gain for a given data and hypothesis (passed in as a string - x and y).  Information Gain can be represented as: Infgain(Y|X_K) = H(Y) - H(Y|X_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Data processed\n",
      "-----------------\n",
      "\t120 = # Of People with Heart Disease\n",
      "\t150 = # Of People without Heart Disease\n"
     ]
    }
   ],
   "source": [
    "# Arrays for the Data instances\n",
    "#     absence_heart_array  : contains all Data instantiations where heart disease is absent\n",
    "#     presence_heart_array : contains all Data instantiations where heart disease is absent\n",
    "absence_heart_array = []\n",
    "presence_heart_array = []\n",
    "total_data_array = []\n",
    "\n",
    "# Classify the features according to their type: nominal or continuous\n",
    "indices_for_nominal = [10, 1, 5, 8, 6, 2, 12]\n",
    "indices_for_continuous = [0,3,4,7,9,11]\n",
    "feature_names = [\"age\", \"sex\", \"chest_pain_type\", \"resting_blood_pressure\", \"serum_cholesterol\", \"fasting_blood_sugar\",\n",
    "                \"resting_electrocardiographic_results\", \"maximum_heart_rate_achieved\", \"exercise_induced_angina\",\n",
    "                \"oldpeak\", \"slope_peak_exercise\", \"number_of_major_vessels\", \"thal\", \"has_heart_disease\"]\n",
    "features_and_types = OrderedDict()\n",
    "for feature in feature_names:\n",
    "    if feature_names.index(feature) in indices_for_continuous:\n",
    "        features_and_types[feature] = \"continuous\"\n",
    "    else:\n",
    "        features_and_types[feature] = \"nominal\"\n",
    "\n",
    "# Process the data and store it in the arrays\n",
    "data = open('heart.data.txt')\n",
    "for line in data.readlines():\n",
    "    feature_value_list = line.split()\n",
    "    has_heart_disease = int(feature_value_list[-1])\n",
    "    data = Data(has_heart_disease)\n",
    "    counter = 0\n",
    "    feature_dict = OrderedDict()\n",
    "    for feature in feature_value_list:\n",
    "        data.data_vars[feature_names[counter]] = float(feature)\n",
    "        counter += 1\n",
    "    if has_heart_disease == 2:\n",
    "        presence_heart_array.append(data)\n",
    "    elif has_heart_disease == 1:\n",
    "        absence_heart_array.append(data)\n",
    "    total_data_array.append(data)\n",
    "\n",
    "presence_heart_array_num = len(presence_heart_array)\n",
    "absence_heart_array_num = len(absence_heart_array)\n",
    "print \"✔ Data processed\"\n",
    "print \"-----------------\"\n",
    "print \"\\t\" + str(presence_heart_array_num) + \" = # Of People with Heart Disease\"\n",
    "print \"\\t\" + str(absence_heart_array_num) + \" = # Of People without Heart Disease\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_info_gain(y, x, buckets):\n",
    "    print \"compute_info_gain called for buckets: \"\n",
    "    print buckets\n",
    "    \n",
    "    # Define dicts for the counts\n",
    "    positive_y_counts = {}\n",
    "    positive_x_counts = {}\n",
    "    negative_y_counts = {}\n",
    "    negative_x_counts = {}\n",
    "    \n",
    "    # Get the bucket values\n",
    "    for bucket in buckets:\n",
    "        # Convert to string\n",
    "        s = \"\"\n",
    "        for num in list(set(bucket)):\n",
    "            s += (str(num)+ \" \")\n",
    "        positive_x_counts[s] = 0\n",
    "        negative_x_counts[s] = 0        \n",
    "\n",
    "    \n",
    "    y_denom = 0\n",
    "    for data in presence_heart_array:\n",
    "        y_denom += 1\n",
    "        value = data.data_vars[y]\n",
    "        \n",
    "        # Value is not in dictionary yet\n",
    "        # so set the occurence for that value to 1\n",
    "        if value not in positive_y_counts.keys():\n",
    "            positive_y_counts[value] = 1 \n",
    "        # Value is already in dictionary\n",
    "        # so increase the occurence count for that value by 1\n",
    "        else:\n",
    "            current_count_for_value = positive_y_counts[value]\n",
    "            positive_y_counts.update({value:current_count_for_value+1})\n",
    "            \n",
    "        # Same thing is done for processing x:\n",
    "        x_value = data.data_vars[x]\n",
    "        for key in negative_x_counts.keys():\n",
    "            if str(x_value)+\" \" in key:\n",
    "                current_count_for_value = positive_x_counts[key]\n",
    "                positive_x_counts.update({key:current_count_for_value+1})\n",
    "            \n",
    "    \n",
    "    for data in absence_heart_array:\n",
    "        y_denom += 1\n",
    "        value = data.data_vars[y]\n",
    "        \n",
    "        # Value is not in dictionary yet\n",
    "        # so set the occurence for that value to 1\n",
    "        if value not in negative_y_counts.keys():\n",
    "            negative_y_counts[value] = 1 \n",
    "        # Value is already in dictionary\n",
    "        # so increase the occurence count for that value by 1\n",
    "        else:\n",
    "            current_count_for_value = negative_y_counts[value]\n",
    "            negative_y_counts.update({value:current_count_for_value+1})\n",
    "            \n",
    "        # Same thing is done for processing x:\n",
    "        x_value = data.data_vars[x]\n",
    "        for key in negative_x_counts.keys():\n",
    "            if str(x_value)+\" \" in key:\n",
    "                current_count_for_value = negative_x_counts[key]\n",
    "                negative_x_counts.update({key:current_count_for_value+1})\n",
    "            \n",
    "            \n",
    "    # Calculate H(Y)     \n",
    "    h_of_y = 0\n",
    "    for count in positive_y_counts.values():\n",
    "        p = float(float(count)/float(y_denom))\n",
    "        entropy = -1 * p * (math.log(p, 2))\n",
    "        h_of_y += entropy\n",
    "    for count in negative_y_counts.values():\n",
    "        p = float(float(count)/float(y_denom))\n",
    "        entropy = -1 * p * (math.log(p, 2))\n",
    "        h_of_y += entropy\n",
    "    \n",
    "    h_of_y_given_x = 0\n",
    "    for feature_value in positive_x_counts.keys():\n",
    "        sum_of_values = positive_x_counts[feature_value] + negative_x_counts[feature_value]\n",
    "        fraction = float(float(sum_of_values)/float(y_denom))\n",
    "        p_positive = float(float(positive_x_counts[feature_value])/float(sum_of_values)) \n",
    "        p_negative = float(float(negative_x_counts[feature_value])/float(sum_of_values)) \n",
    "        entropy_positive = -p_positive * (math.log(p_positive, 2))\n",
    "        entropy_negative = -p_negative * (math.log(p_negative, 2))\n",
    "        h_of_y_given_x -= fraction*(entropy_positive+entropy_negative)\n",
    "    \n",
    "    info_gain = h_of_y - h_of_y_given_x\n",
    "    return info_gain\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Function used to generate all of the possible splits\n",
    "Credit:  https://stackoverflow.com/questions/19368375/set-partitions-in-python/30134039#30134039"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def partition(collection):\n",
    "    if len(collection) == 1:\n",
    "        yield [ collection ]\n",
    "        return\n",
    "\n",
    "    first = collection[0]\n",
    "    for smaller in partition(collection[1:]):\n",
    "        # insert `first` in each of the subpartition's subsets\n",
    "        for n, subset in enumerate(smaller):\n",
    "            yield smaller[:n] + [[ first ] + subset]  + smaller[n+1:]\n",
    "        # put `first` in its own subset \n",
    "        yield [ [ first ] ] + smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Determine possible splits\n",
    "These will be used for the information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get_continuous_binary_split(feature):     \n",
    "#    Gets a feature and splits the data for that feature in all the possible ways to split that\n",
    "#    data into two buckets.  It returns a dictionary where the key is the number it split on and the\n",
    "#    value is a list of two lists.  The first element of the list is all of the elements less than\n",
    "#    the split and the second element of the list is a list of all of the elements greater than or\n",
    "#    equal to the split.\n",
    "#    For example:\n",
    "#           feature = \"age\"\n",
    "#           splits_dict = {50: [[20,40,43...],[50,60,61...]], 60: [[57,45,59...],[60,61,63...]]}\n",
    "def get_continuous_binary_split(feature):\n",
    "    # Ensure the function is being called only with continuous features\n",
    "    if features_and_types[feature] != \"continuous\":\n",
    "        raise ValueError('Error in get_continuous_binary_split: input feature is not continuous.')\n",
    "        return\n",
    "    \n",
    "    # Create a list of the possible splits\n",
    "    splits = []\n",
    "    for data in total_data_array:\n",
    "        feature_value = float(data.data_vars[feature])\n",
    "        if feature_value not in splits:\n",
    "            splits.append(feature_value)\n",
    "    \n",
    "    # Process the splits and create the less than list and greater than or equal to list for each\n",
    "    splits_dict = {}\n",
    "    for split in splits:\n",
    "        lt_split_feature_values = []\n",
    "        gtequal_split_feature_values = []\n",
    "        for data in total_data_array:\n",
    "            feature_value = float(data.data_vars[feature])\n",
    "            if feature_value < split:\n",
    "                lt_split_feature_values.append(feature_value)\n",
    "            else:\n",
    "                gtequal_split_feature_values.append(feature_value)\n",
    "        splits_dict[split] = [lt_split_feature_values, gtequal_split_feature_values]\n",
    "    \n",
    "    return splits_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "# get_nominal_split(feature):     \n",
    "#    Gets a nominal feature and splits the data for that feature in all the possible ways to split that\n",
    "#    data into every possible number of buckets.  It returns a dictionary where the key is the number it \n",
    "#    split on and the value is a list of the lists it generated from the split.  \n",
    "\n",
    "def split_list(data, n):\n",
    "    from itertools import combinations, chain\n",
    "    for splits in combinations(range(1, len(data)), n-1):\n",
    "        result = []\n",
    "        prev = None\n",
    "        for split in chain(splits, [None]):\n",
    "            result.append(data[prev:split])\n",
    "            prev = split\n",
    "        yield result\n",
    "        \n",
    "def get_nominal_split(feature):\n",
    "    # Ensure the function is being called only with nominal features\n",
    "    if features_and_types[feature] != \"nominal\":\n",
    "        raise ValueError('Error in get_nominal_split: input feature is not nominal.')\n",
    "        return\n",
    "    \n",
    "    # Determine the unique values for the feature and the counts for the feature_value\n",
    "    splits = {}\n",
    "    split_nums = []\n",
    "    for data in total_data_array:\n",
    "        feature_value = float(data.data_vars[feature])\n",
    "        if feature_value not in splits:\n",
    "            split_nums.append(feature_value)\n",
    "            splits[feature_value] = 1\n",
    "        else:\n",
    "            current_num = splits[feature_value]\n",
    "            splits[feature_value] = current_num+1\n",
    "\n",
    "    # Generate all the possible ways to partition the numbers  \n",
    "    # possible_partitions is a list of tuples where the first element of the tuple\n",
    "    # is the kth possible partition and the second element of the tuple is a list\n",
    "    # of lists of partitions\n",
    "    possible_partitions = []\n",
    "    for p in partition(split_nums):\n",
    "        # Be sure to remove the split where all values are placed in one bucket\n",
    "        if len(sorted(p)) != 1:\n",
    "            possible_partitions.append(sorted(p))\n",
    "                \n",
    "    # Generate a list of partition dicts\n",
    "    # Each list element will be a list of dictionaries\n",
    "    # The dictionaries will contain the feature_value and the number of times it occurs\n",
    "    # The list of these dictionaries will be split up by partition\n",
    "    partitions_and_values = []\n",
    "    for split_lists in possible_partitions:\n",
    "        split_dicts = []\n",
    "        for bucket in split_lists:\n",
    "            bucket_dict = {}\n",
    "            for feature_value in bucket:\n",
    "                num_of_feature_value_occurences = splits[feature_value]\n",
    "                bucket_dict[feature_value] = num_of_feature_value_occurences\n",
    "            split_dicts.append(bucket_dict)\n",
    "        partitions_and_values.append(split_dicts)\n",
    "    return partitions_and_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_best_split(y, feature):\n",
    "    # Check if the feature is nominal or continuous and split accordingly\n",
    "    splits = []\n",
    "    if features_and_types[feature] == \"nominal\":\n",
    "        splits = get_nominal_split(feature)\n",
    "    else:\n",
    "        splits = get_continuous_split(feature)\n",
    "    \n",
    "    # Create a dictionary where the key is the information gain from a particular\n",
    "    # split and the value is that particular split\n",
    "    info_gains = {}\n",
    "    for split in splits:\n",
    "        info_gain = compute_info_gain(y, feature, split)\n",
    "        info_gains[info_gain] = split\n",
    "    \n",
    "    # Process the dictionary and determine the highest info gain\n",
    "    max_info_gain = max(info_gains, key=float)\n",
    "    split_yielding_max_info_gan = info_gains[max_info_gain]\n",
    "    \n",
    "    # Print the maximum info gain and corresponding split\n",
    "    return {\"Greatest Info Gain\":max_info_gain,\n",
    "           \"from split\":split_yielding_max_info_gan}\n",
    "    \n",
    "        \n",
    "    '''\n",
    "    info_gains = {}\n",
    "    max_split_info = 0\n",
    "    split_on_value = 0\n",
    "    for split in splits.keys():\n",
    "        info_gains[split] = compute_info_gain(\"has_heart_disease\", feature, splits[split], len(splits[split]))\n",
    "        print str(info_gains[split])+\" = info gain for splitting on \"+str(split)\n",
    "        if info_gains[split] > max_split_info:\n",
    "            max_split_info = info_gains[split]\n",
    "            split_on_value = split\n",
    "    return {\"Greatest Info Gain\":max_split_info,\n",
    "           \"from splitting on value\":split_on_value}\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Recurse to build the tree\n",
    "Used the information gain function to determine the best splits for each node of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_tree():\n",
    "    for feature in data.data_vars.keys():\n",
    "        print compute_best_split(\"has_heart_disease\", \"thal\")\n",
    "        #information_gain = info_gain(\"has_heart_disease\", feature)\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[3.0], [7.0, 6.0]], [[3.0, 7.0], [6.0]], [[3.0, 6.0], [7.0]], [[3.0], [6.0], [7.0]]]\n",
      "compute_info_gain called for buckets: \n",
      "[{3.0: 152}, {6.0: 14, 7.0: 104}]\n",
      "compute_info_gain called for buckets: \n",
      "[{3.0: 152, 7.0: 104}, {6.0: 14}]\n",
      "compute_info_gain called for buckets: \n",
      "[{3.0: 152, 6.0: 14}, {7.0: 104}]\n",
      "compute_info_gain called for buckets: \n",
      "[{3.0: 152}, {6.0: 14}, {7.0: 104}]\n",
      "{'Greatest Info Gain': 1.9795954762320551, 'from split': [{3.0: 152, 7.0: 104}, {6.0: 14}]}\n"
     ]
    }
   ],
   "source": [
    "build_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
